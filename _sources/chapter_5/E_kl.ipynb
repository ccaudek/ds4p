{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d78e3f-979f-467f-af25-f72a0a62b4ef",
   "metadata": {},
   "source": [
    "# ✏️ Esercizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a6d63e-63f2-474d-b693-90e0fbe6c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d40def-1d56-4ac1-ad24-feb3c4c7914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 8927\n",
    "rng = np.random.default_rng(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e50f0-9e08-4cdc-bd0b-31fe40585ad9",
   "metadata": {},
   "source": [
    "## Una Misura della Perdita di Informazione\n",
    "\n",
    "La Divergenza di Kullback-Leibler ($\\mathbb{KL}$), conosciuta anche come entropia relativa, funge da misura quantitativa della perdita di informazione che si verifica quando una distribuzione di probabilità complessa o sconosciuta, denominata $p$, viene approssimata con una distribuzione più semplice, chiamata $q$. Questa misura ci fornisce una valutazione di quanto le due distribuzioni differiscano l'una dall'altra in termini informativi. Dal punto di vista matematico, la $\\mathbb{KL}$ si calcola sommando le differenze tra le probabilità logaritmiche associate ad ogni evento possibile secondo $p$ e $q$, ponderate per le probabilità degli eventi secondo $p$. La formula generale è:\n",
    "\n",
    "$$\n",
    "\\mathbb{KL}(p \\parallel q) = \\sum_{i=1}^n p_i (\\log p_i - \\log q_i),\n",
    "$$\n",
    "\n",
    "dove $i$ indica ciascun possibile evento all'interno delle distribuzioni. Questo calcolo produce una misura della \"distanza\" media tra $p$ e $q$, considerando le loro probabilità logaritmiche.\n",
    "\n",
    "### Un Esempio Empirico\n",
    "\n",
    "Per comprendere meglio questi concetti, esaminiamo ora un esempio numerico, dove definiremo due distribuzioni di probabilità discrete, $p$ e $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5263a6ef-b5d3-454f-9f0e-05d18d036c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([0.2, 0.5, 0.3])\n",
    "q = np.array([0.1, 0.2, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df9c2f-00e4-4fd4-bf23-cfada21de14b",
   "metadata": {},
   "source": [
    "Calcoliamo l'entropia di $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8103b3d-959a-4a0c-b863-3637a8e4986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropia di p:  1.0296530140645737\n"
     ]
    }
   ],
   "source": [
    "h_p = -np.sum(p * np.log(p))\n",
    "print(\"Entropia di p: \", h_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058261c-e8b9-4b23-9b82-966116cdadb0",
   "metadata": {},
   "source": [
    "Calcoliamo l'entropia incrociata tra $p$ e $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87c648b0-69a6-4b1d-b3b9-5588d325677c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropia incrociata tra p e q:  1.372238457997479\n"
     ]
    }
   ],
   "source": [
    "h_pq = -np.sum(p * np.log(q))\n",
    "print(\"Entropia incrociata tra p e q: \", h_pq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf4167-3160-42c5-8e98-04b5d21b039e",
   "metadata": {},
   "source": [
    "Calcoliamo la divergenza di Kullback-Leibler da $p$ a $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c096d83-4d9f-4855-a2ca-e49261537726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergenza KL da p a q:  0.34258544393290524\n"
     ]
    }
   ],
   "source": [
    "kl_pq = h_pq - h_p\n",
    "print(\"Divergenza KL da p a q: \", kl_pq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d8e5b3-c035-40b3-a447-42164c8c8cc7",
   "metadata": {},
   "source": [
    "Lo stesso risultato si ottiene applicando la formula della Divergenza $\\mathbb{KL}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a170186-81d5-464a-ba6a-eb1d80d654e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3425854439329054"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(p * (np.log(p) - np.log(q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86393e00-4f03-47d8-80db-2164ccb02ccc",
   "metadata": {},
   "source": [
    "Se invece $q$ è molto simile a $p$, la differenza $\\mathbb{KL}$ è molto minore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b59cdb91-2305-4099-89ce-3d7149755bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007041377136023895"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.array([0.2, 0.5, 0.3])\n",
    "q = np.array([0.2, 0.55, 0.25])\n",
    "np.sum(p * (np.log(p) - np.log(q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d4749-e171-41ae-94a2-dcf57fd47a1a",
   "metadata": {},
   "source": [
    "Consideriamo un secondo esempio. Sia $p$ una distribuzione binomiale di parametri $\\theta = 0.2$ e $n = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7602b929-0a83-483a-9300-19934376b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4096 0.4096 0.1536 0.0256 0.0016]\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "n = 4\n",
    "p = 0.2\n",
    "\n",
    "# Compute the probability mass function\n",
    "true_py = stats.binom.pmf(range(n + 1), n, p)\n",
    "print(true_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f43ac8-7a74-474a-96ad-05ba34125b78",
   "metadata": {},
   "source": [
    "Sia $q_1$ una approssimazione a $p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4139d5c6-b8b4-4ad0-b440-a229829a0729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46 0.42 0.1  0.01 0.01]\n"
     ]
    }
   ],
   "source": [
    "q1 = np.array([0.46, 0.42, 0.10, 0.01, 0.01])\n",
    "print(q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f11ad-5a7c-46de-ab16-2cfa779e26a6",
   "metadata": {},
   "source": [
    "Sia $q_2$ una distribuzione uniforme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bf3fce2-1a03-46a8-b8b7-8e914ff0034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.2, 0.2, 0.2, 0.2]\n"
     ]
    }
   ],
   "source": [
    "q2 = [0.2] * 5\n",
    "print(q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f5cf0-13d5-4b4e-be93-47886645fddc",
   "metadata": {},
   "source": [
    "La divergenza $\\mathbb{KL}$ di $q_1$ da $p$ è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe62a3b6-3d8b-4729-b86e-a918380d01a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergenza KL di q1 da p:  0.02925199033345882\n"
     ]
    }
   ],
   "source": [
    "kl_pq1 = np.sum(true_py * (np.log(true_py) - np.log(q1)))\n",
    "print(\"Divergenza KL di q1 da p: \", kl_pq1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aebe8b-0a1d-4bd1-bd87-fecd4bbc556d",
   "metadata": {},
   "source": [
    "La divergenza $\\mathbb{KL}$ di $q_2$ da $p$ è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da757218-fd82-4999-8479-cfd0f3327752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergenza KL di q2 da p:  0.48635777871415425\n"
     ]
    }
   ],
   "source": [
    "kl_pq2 = np.sum(true_py * (np.log(true_py) - np.log(q2)))\n",
    "print(\"Divergenza KL di q2 da p: \", kl_pq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e1635-1dca-40d7-a89d-7e5a9d678876",
   "metadata": {},
   "source": [
    "È chiaro che perdiamo una quantità maggiore di informazioni se, per descrivere la distribuzione binomiale $p$, usiamo la distribuzione uniforme $q_2$ anziché $q_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c36604-169d-438f-8ff9-0402c1895d7b",
   "metadata": {},
   "source": [
    "### La Divergenza Dipende dalla Direzione\n",
    "\n",
    "La Divergenza $\\mathbb{KL}$ è spesso paragonata a una \"distanza\" tra due distribuzioni di probabilità, ma è fondamentale capire che non è simmetrica. Questo significa che la misura di quanto $p$ è diversa da $q$ non è la stessa di quanto $q$ è diversa da $p$. Questa asimmetria riflette la differenza nella perdita di informazione quando si sostituisce una distribuzione con l'altra.\n",
    "\n",
    "Per comprendere meglio la Divergenza $\\mathbb{KL}$, è utile considerare l'entropia e l'entropia incrociata. Facciamo un esempio numerico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d79c1d0a-0209-47fe-96a9-0a4423abbae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropia di p: 0.056001534354847345\n",
      "Entropia incrociata da p a q: 1.1954998257220641\n",
      "Divergenza KL da p a q: 1.1394982913672167\n",
      "\n",
      "Entropia di q: 0.6108643020548935\n",
      "Entropia incrociata da q a p: 3.226634230947714\n",
      "Divergenza KL da q a p: 2.6157699288928207\n"
     ]
    }
   ],
   "source": [
    "# Definire le distribuzioni p e q\n",
    "p = np.array([0.01, 0.99])\n",
    "q = np.array([0.7, 0.3])\n",
    "\n",
    "# Calcolo dell'entropia di p\n",
    "h_p = -np.sum(p * np.log(p))\n",
    "\n",
    "# Calcolo dell'entropia incrociata da p a q\n",
    "h_pq = -np.sum(p * np.log(q))\n",
    "\n",
    "# Calcolo della divergenza KL da p a q\n",
    "kl_pq = h_pq - h_p\n",
    "\n",
    "# Calcolo dell'entropia di q\n",
    "h_q = -np.sum(q * np.log(q))\n",
    "\n",
    "# Calcolo dell'entropia incrociata da q a p\n",
    "h_qp = -np.sum(q * np.log(p))\n",
    "\n",
    "# Calcolo della divergenza KL da q a p\n",
    "kl_qp = h_qp - h_q\n",
    "\n",
    "print(f\"Entropia di p: {h_p}\")\n",
    "print(f\"Entropia incrociata da p a q: {h_pq}\")\n",
    "print(f\"Divergenza KL da p a q: {kl_pq}\")\n",
    "\n",
    "print(f\"\\nEntropia di q: {h_q}\")\n",
    "print(f\"Entropia incrociata da q a p: {h_qp}\")\n",
    "print(f\"Divergenza KL da q a p: {kl_qp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa301c-d21b-43fb-9815-643786969f36",
   "metadata": {},
   "source": [
    "- **Entropia di $p$ ($h(p)$)**: Misura l'incertezza o la variabilità all'interno della distribuzione vera $p$. Nel nostro esempio, l'entropia di $p$ è 0.056.\n",
    "- **Entropia Incrociata da $p$ a $q$ ($h(p, q)$)**: Misura l'incertezza quando si usa $q$ per rappresentare $p$. Nel nostro esempio, è 1.195.\n",
    "\n",
    "La Divergenza $\\mathbb{KL}$ da $p$ a $q$ si calcola come la differenza tra l'entropia incrociata e l'entropia di $p$, che nel nostro caso è 1.139. Questo valore indica la perdita di informazione quando si utilizza $q$ per approssimare $p$.\n",
    "\n",
    "Quando invertiamo le distribuzioni e calcoliamo la Divergenza $\\mathbb{KL}$ da $q$ a $p$, otteniamo un risultato diverso. L'entropia di $q$ è 0.611 e l'entropia incrociata da $q$ a $p$ è 3.227. La Divergenza $\\mathbb{KL}$ risultante da $q$ a $p$ è 2.616, molto più grande rispetto a quella da $p$ a $q$.\n",
    "\n",
    "Questi risultati dimostrano chiaramente che la Divergenza $\\mathbb{KL}$ è asimmetrica. La quantità di informazione che si perde nel sostituire $p$ con $q$ non è la stessa che si perde sostituendo $q$ con $p$. Questa asimmetria è una caratteristica cruciale della Divergenza $\\mathbb{KL}$ e sottolinea l'importanza di considerare attentamente quale distribuzione si sta utilizzando come approssimazione dell'altra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50258a58-eb65-4d8c-b897-f70286917052",
   "metadata": {},
   "source": [
    "## Informazioni sull'Ambiente di Sviluppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83d166de-a53b-45f2-8699-228aa011d401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Last updated: Wed Apr 24 2024\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.12.2\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "numpy      : 1.26.4\n",
      "pandas     : 2.2.2\n",
      "statsmodels: 0.14.1\n",
      "matplotlib : 3.8.4\n",
      "pymc       : 5.13.0\n",
      "arviz      : 0.18.0\n",
      "seaborn    : 0.13.2\n",
      "scipy      : 1.13.0\n",
      "\n",
      "Watermark: 2.4.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
