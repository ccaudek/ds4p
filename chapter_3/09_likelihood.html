
<!DOCTYPE html>


<html lang="it" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>La verosimiglianza &#8212; ds4p</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css?v=20b57f81" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=8d586cc4"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/translations.js?v=0173e136"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-TP2WLBPMS6"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-TP2WLBPMS6');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-TP2WLBPMS6');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_3/09_likelihood';</script>
    <link rel="canonical" href="https://ccaudek.github.io/ds4p/chapter_3/09_likelihood.html" />
    <link rel="icon" href="../_static/increasing.png"/>
    <link rel="index" title="Indice" href="../genindex.html" />
    <link rel="search" title="Cerca" href="../search.html" />
    <link rel="next" title="✏️ Esercizi" href="E_likelihood.html" />
    <link rel="prev" title="✏️ Esercizi" href="E_beta_distr.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="it"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Passa ai contenuti principali</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Torna in alto</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Avvertimento sulla versione"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="ds4p - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="ds4p - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Cerca" aria-label="Cerca" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Cerca</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Benvenuti
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_1/introduction_chapter_1.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/00_prelims.html">Preliminari</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/01_python_1.html">Python (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/02_python_2.html">Python (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_python.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/03_numpy.html">NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_numpy.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/04_pandas.html">Pandas (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/05_pandas_aggregate.html">Pandas (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/06_pandas_functions.html">Pandas (3)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_pandas.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/07_matplotlib.html">Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/08_seaborn.html">Seaborn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_matplotlib.html">✏️ Esercizi</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_2/introduction_chapter_2.html">Statistica descrittiva</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/00_scientific_method.html">La scienza dei dati e il metodo scientifico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/01_key_notions.html">Concetti chiave</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_key_notions.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/02_measurement.html">La misurazione in psicologia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_scales.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/03_freq_distr.html">Dati e frequenze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_sums.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/04_loc_scale.html">Indici di posizione e di scala</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/05_correlation.html">Le relazioni tra variabili</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/06_causality.html">Lo studio delle cause dei fenomeni</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_eda.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_mehr_song_spelke.html">✏️ Esercizi</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="introduction_chapter_3.html">Probabilità</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_intro_prob.html">Introduzione al calcolo delle probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_conditional_prob.html">Probabilità condizionata</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_cond_prob_1.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_cond_prob_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_discrete_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_bayes_theorem.html">Il teorema di Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_bayes_theorem.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_bayes_theorem_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="04a_random_var.html">Introduzione alle variabili casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="04b_expval_var.html">Proprietà delle variabili casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_exp_val_variance.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="04c_sampling_distr.html">Stime, stimatori e parametri</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_rv_discrete.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_joint_prob.html">Probabilità congiunta</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_joint_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_covariance.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_density_func.html">La funzione di densità di probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_discr_rv_distr.html">Distribuzioni di v.c. discrete</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_binomial.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="08_cont_rv_distr.html">Distribuzioni di v.c. continue</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_gaussian.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_beta_distr.html">✏️ Esercizi</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">La verosimiglianza</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_likelihood.html">✏️ Esercizi</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_4/introduction_part_4.html">Inferenza bayesiana</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/01_intro_bayes.html">Modellazione bayesiana</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/02_subj_prop.html">Pensare ad una proporzione in termini soggettivi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/02_grid_gauss.html">Verosimiglianza Gaussiana: Metodo Basato su Griglia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/03_conjugate_families_1.html">Distribuzioni coniugate</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_conjugate_families_1.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/04_conjugate_families_2.html">Distribuzioni coniugate (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/05_summary_posterior.html">Sintesi a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_conjugate.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/06_balance-prior-post.html">L’influenza della distribuzione a priori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/10_metropolis.html">Monte Carlo a Catena di Markov</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/15_stan_beta_binomial.html">Introduzione a Stan</a></li>

<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_stan_beta_binomial.html">✏️ Esercizio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/16_stan_summary_posterior.html">Metodi di sintesi della distribuzione a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/17_stan_diagnostics.html">Diagnostica delle catene markoviane</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/18_stan_prediction.html">La predizione bayesiana</a></li>

<li class="toctree-l2"><a class="reference internal" href="../chapter_4/19_stan_odds_ratio.html">Analisi bayesiana dell’odds-ratio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/22_stan_normal_normal.html">Inferenza bayesiana su una media</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/23_stan_two_groups.html">Confronto tra due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/24_stan_hier_beta_binom.html">Modello gerarchico beta-binomiale con Stan</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_5/introduction_part_5.html">Analisi della regressione</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_03_reglin_bayesian.html">Analisi bayesiana del modello di regressione lineare bivariato</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_1.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_04_synt_sugar.html">Zucchero sintattico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_05_two_means.html">Confronto tra le medie di due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_3.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_4.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_06_hier_regr.html">Il modello lineare gerarchico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_07_robust_regr.html">Regressione robusta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_08_specification_error.html">Errore di specificazione</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_09_causal_inference.html">Inferenza causale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_causal_inference.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_22_stan_logistic_regr.html">Regressione logistica con Stan</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_24_stan_mixed_models.html">Modelli misti con Stan</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_25_stan_rct.html">Incorporare dati storici di controllo in una RCT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_26_stan_mediation.html">Modello di mediazione con Stan</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_30_entropy.html">Entropia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_31_kl.html">La divergenza di Kullback-Leibler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_kl.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_32_stan_loo.html">Validazione Incrociata Leave-One-Out</a></li>

<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_35_missing.html">Dati mancanti</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_40_rescorla_wagner.html">Apprendimento per rinforzo</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_6/introduction_part_6.html">Inferenza frequentista</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/01_intro_frequentist.html">Introduzione all’inferenza frequentista</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_estimation.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/02_conf_interv.html">Intervallo di confidenza</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_conf_interv.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/03_test_ipotesi.html">Significatività statistica</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_t_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_interpretation_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_significato_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/04_two_ind_samples.html">Test t di Student per campioni indipendenti</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_test_media_pop.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_ampie.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_piccoli.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_campioni_appaiati.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_confronto_proporzioni.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/05_crisis.html">La crisi della generalizzabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/06_limiti_stat_frequentista.html">Limiti dell’inferenza frequentista</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/07_effect_size.html">La grandezza dell’effetto</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/09_s_m_errors.html">Crisi della replicabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/10_integrity.html">Integrità della ricerca</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../references/bibliography.html">Bibliografia</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_7/introduction_appendix.html">Appendici</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a00_installation.html">Ambiente di lavoro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a01_markdown.html">Jupyter Notebook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a02_shell.html">La Shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a03_colab_tutorial.html">Colab: un breve tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a04_virtual_env.html">Ambienti virtuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a10_math_symbols.html">Simbologia di base</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a11_numbers.html">Numeri binari, interi, razionali, irrazionali e reali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a12_sum_notation.html">Simbolo di somma (sommatorie)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a13_sets.html">Insiemi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a13a_probability.html">Sigma algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a14_combinatorics.html">Calcolo combinatorio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a15_calculus.html">Per liberarvi dai terrori preliminari</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a20_kde_plot.html">Kernel Density Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a30_prob_tutorial.html">Esercizi di probabilità discreta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a40_rng.html">Generazione di numeri casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a44_montecarlo.html">Simulazione Monte Carlo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a45_mcmc.html">Catene di Markov</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a46_stan.html">Linguaggio Stan</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a50_lin_fun.html">La funzione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a50_reglin_ml.html">Modello di Regressione Bivariato e ML</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a51_reglin_1.html">Regressione lineare bivariata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a60_ttest_exercises.html">Esercizi sull’inferenza frequentista</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a70_predict_counts.html">La predizione delle frequenze</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/ccaudek/ds4p/blob/main/docs/chapter_3/09_likelihood.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Scarica questa pagina">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_3/09_likelihood.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Scarica il file sorgente"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Stampa in PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Modalità schermo intero"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="chiaro/scuro" aria-label="chiaro/scuro" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Cerca" aria-label="Cerca" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>La verosimiglianza</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contenuti </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparazione-del-notebook">Preparazione del Notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#il-principio-della-verosimiglianza-e-la-sua-formalizzazione">Il Principio della Verosimiglianza e la sua Formalizzazione</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verosimiglianza-binomiale">Verosimiglianza Binomiale</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretazione-della-funzione-di-verosimiglianza">Interpretazione della Funzione di Verosimiglianza</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#massima-verosimiglianza">Massima verosimiglianza</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-funzione-di-log-verosimiglianza">La Funzione di Log-Verosimiglianza</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#verosimiglianza-congiunta">Verosimiglianza Congiunta</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-verosimiglianza-marginale">La Verosimiglianza Marginale</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modello-gaussiano-e-verosimiglianza">Modello Gaussiano e Verosimiglianza</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#caso-di-una-singola-osservazione">Caso di una Singola Osservazione</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana">Campione indipendente di osservazioni da una distribuzione gaussiana</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-stima-di-massima-verosimiglianza-per-mu">La Stima di Massima Verosimiglianza per <span class="math notranslate nohighlight">\( \mu \)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusione-e-riflessioni-finali">Conclusione e Riflessioni Finali</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#informazioni-sull-ambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="la-verosimiglianza">
<span id="notebook-likelihood"></span><h1>La verosimiglianza<a class="headerlink" href="#la-verosimiglianza" title="Link to this heading">#</a></h1>
<p>Oltre agli approcci frequentisti e bayesiani, esiste un terzo metodo fondamentale nell’ambito dell’inferenza statistica: la metodologia basata sulla verosimiglianza. Questo approccio consente ai ricercatori di valutare l’evidenza relativa quando si confrontano due modelli o ipotesi, in maniera simile alla metodologia bayesiana. Ciò che lo distingue è il suo esplicito rifiuto di incorporare informazioni pregresse (priori) nelle analisi statistiche.</p>
<p>Questo capitolo si concentra sulla funzione di verosimiglianza, concetto centrale che si estende attraverso tutti e tre gli approcci statistici, fungendo da collegamento tra i dati osservati e i parametri di un modello statistico specifico. La rilevanza della funzione di verosimiglianza risiede nella sua capacità di fornire un fondamento robusto per l’interpretazione e la quantificazione dell’adeguatezza dei dati ai modelli teorici. Questo la rende uno strumento cruciale per l’inferenza statistica, indispensabile per comprendere e valutare la conformità dei dati rispetto alle teorie proposte.</p>
<div class="admonition-obiettivi-di-apprendimento admonition">
<p class="admonition-title">Obiettivi di apprendimento</p>
<p>Dopo aver completato questo capitolo, sarete in grado di:</p>
<ul class="simple">
<li><p>Comprendere il concetto di verosimiglianza e il suo ruolo nella dei parametri.</p></li>
<li><p>Generare grafici della funzione di verosimiglianza binomiale.</p></li>
<li><p>Generare grafici della funzione di verosimiglianza del modello gaussiano.</p></li>
<li><p>Interpretare i grafici della funzione di verosimiglianza.</p></li>
<li><p>Comprendere il concetto di stima di massima verosimiglianza.</p></li>
</ul>
</div>
<section id="preparazione-del-notebook">
<h2>Preparazione del Notebook<a class="headerlink" href="#preparazione-del-notebook" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">scipy.integrate</span> <span class="kn">import</span> <span class="n">quad</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># set seed to make the results fully reproducible</span>
<span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">ord</span><span class="p">,</span> <span class="s2">&quot;likelihood&quot;</span><span class="p">))</span>
<span class="n">rng</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">Generator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;arviz-darkgrid&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.dpi&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.facecolor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;white&quot;</span>

<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &quot;retina&quot;
</pre></div>
</div>
</div>
</div>
</section>
<section id="il-principio-della-verosimiglianza-e-la-sua-formalizzazione">
<h2>Il Principio della Verosimiglianza e la sua Formalizzazione<a class="headerlink" href="#il-principio-della-verosimiglianza-e-la-sua-formalizzazione" title="Link to this heading">#</a></h2>
<p>La funzione di verosimiglianza e la funzione di densità (o massa) di probabilità sono due concetti fondamentali in statistica che, nonostante condividano la stessa espressione matematica, rivestono ruoli e interpretazioni distinti a seconda del contesto in cui vengono applicati. La chiave per distinguere tra i due concetti risiede nel modo in cui trattiamo i dati e i parametri del modello.</p>
<p>Nel caso della funzione di densità (o massa) di probabilità, i parametri del modello sono fissati e l’obiettivo è valutare la probabilità di osservare un certo insieme di dati. Qui, i dati sono variabili, mentre i parametri sono considerati costanti. Per esempio, in un esperimento in cui lanciamo una moneta diverse volte, potremmo usare una distribuzione binomiale per calcolare la probabilità di ottenere un certo numero di teste, assumendo un valore noto e fisso per la probabilità di ottenere testa in un singolo lancio.</p>
<p>Al contrario, nella funzione di verosimiglianza, manteniamo i dati osservati come fissi e variamo i parametri del modello per valutare quanto bene questi ultimi si adattino ai dati osservati. Questo processo ci permette di esplorare la plausibilità di diversi valori dei parametri dati gli stessi dati. L’obiettivo è identificare il set di parametri che meglio spiega i dati osservati.</p>
<p>Formalmente, la relazione tra la funzione di verosimiglianza e la funzione di densità di probabilità è espressa come segue:</p>
<div class="math notranslate nohighlight">
\[
L(\theta | y) \propto p(y | \theta),
\]</div>
<p>dove <span class="math notranslate nohighlight">\(L(\theta | y)\)</span> rappresenta la funzione di verosimiglianza per i parametri <span class="math notranslate nohighlight">\(\theta\)</span> dati gli osservazioni <span class="math notranslate nohighlight">\(y\)</span>, e <span class="math notranslate nohighlight">\(p(y | \theta)\)</span> indica la probabilità (o densità) di osservare i dati <span class="math notranslate nohighlight">\(y\)</span> dato un certo set di parametri <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Prendiamo l’esempio del lancio di una moneta. Se osserviamo 7 teste su 10 lanci, la funzione di massa di probabilità della distribuzione binomiale ci permette di calcolare la probabilità di questo esito per un dato valore di <span class="math notranslate nohighlight">\(p\)</span> (la probabilità di testa). In questo contesto, <span class="math notranslate nohighlight">\(p\)</span> è fisso e i dati (<span class="math notranslate nohighlight">\(y = 7\)</span> teste in <span class="math notranslate nohighlight">\(n = 10\)</span> lanci) sono variabili.</p>
<p>Dall’altro lato, se consideriamo <span class="math notranslate nohighlight">\(p\)</span> variabile, la funzione di verosimiglianza ci permette di valutare come diversi valori di <span class="math notranslate nohighlight">\(p\)</span> si adattano all’esito osservato di 7 teste su 10 lanci, mantenendo i dati osservati fissi.</p>
<p>È importante sottolineare che, benché le due funzioni condividano la stessa forma matematica, il loro utilizzo e interpretazione sono profondamente diversi. La funzione di densità di probabilità si concentra sulla probabilità degli esiti dati i parametri, mentre la funzione di verosimiglianza valuta la plausibilità dei parametri dati gli esiti. Questa distinzione è cruciale per l’inferenza statistica, permettendoci di stimare i parametri del modello che meglio si adattano ai dati osservati e di comprendere in modo più approfondito la struttura e le caratteristiche del fenomeno studiato.</p>
</section>
<section id="verosimiglianza-binomiale">
<h2>Verosimiglianza Binomiale<a class="headerlink" href="#verosimiglianza-binomiale" title="Link to this heading">#</a></h2>
<p>Proseguendo con l’esempio della distribuzione binomiale, approfondiamo la rilevanza della funzione di verosimiglianza nell’analisi statistica attraverso uno scenario pratico. Supponiamo di condurre un esperimento con un numero definito di prove <span class="math notranslate nohighlight">\(n\)</span>, ognuna delle quali può terminare con un successo o un fallimento, come nel caso dei lanci di una moneta. Se registriamo <span class="math notranslate nohighlight">\(y\)</span> successi e <span class="math notranslate nohighlight">\(n - y\)</span> fallimenti, la probabilità di osservare esattamente <span class="math notranslate nohighlight">\(y\)</span> successi segue la funzione di massa di probabilità (FMP) binomiale, che è definita come:</p>
<div class="math notranslate nohighlight">
\[
P(Y = y) = \binom{n}{y} \theta^y (1 - \theta)^{n - y},
\]</div>
<p>dove <span class="math notranslate nohighlight">\(\theta\)</span> è la probabilità di successo in una singola prova di Bernoulli.</p>
<p>Nell’utilizzo della funzione di verosimiglianza, ci concentriamo su come i diversi valori di <span class="math notranslate nohighlight">\(\theta\)</span> possono spiegare i dati osservati <span class="math notranslate nohighlight">\(y\)</span>. La verosimiglianza è espressa come:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid y) = \theta^y (1 - \theta)^{n - y},
\]</div>
<p>dato che il coefficiente binomiale <span class="math notranslate nohighlight">\(\binom{n}{y}\)</span>, non dipendendo da <span class="math notranslate nohighlight">\(\theta\)</span>, può essere omesso per la semplicità della formulazione.</p>
<div class="exercise admonition" id="ex-likelihood-binomial">

<p class="admonition-title"><span class="caption-number">Exercise 140 </span></p>
<section id="exercise-content">
<p>Per esemplificare, immaginiamo uno studio su un gruppo di 30 individui, di cui 23 presentano un atteggiamento negativo verso il futuro, un indicatore comune in pazienti con depressione <span id="id1">[]</span>. Qui, i nostri dati <span class="math notranslate nohighlight">\(y\)</span> e <span class="math notranslate nohighlight">\(n\)</span> sono fissi, e la funzione di verosimiglianza per <span class="math notranslate nohighlight">\(\theta\)</span> sconosciuto diventa:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} \theta^{23} (1 - \theta)^7.
\]</div>
<p>Valutando questa funzione per una serie di valori di <span class="math notranslate nohighlight">\(\theta\)</span> possiamo determinare quale valore di <span class="math notranslate nohighlight">\(\theta\)</span> rende i dati osservati più verosimili. Procediamo simulando 100 valori equidistanti di <span class="math notranslate nohighlight">\(\theta\)</span> nell’intervallo [0, 1] e calcoliamo la verosimiglianza per ciascuno di questi valori.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">23</span>
</pre></div>
</div>
</div>
</div>
<p>Creiamo i possibili valori del parametro <span class="math notranslate nohighlight">\(\theta\)</span> per i quali calcoleremo la verosimiglianza.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.         0.01010101 0.02020202 0.03030303 0.04040404 0.05050505
 0.06060606 0.07070707 0.08080808 0.09090909 0.1010101  0.11111111
 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717
 0.18181818 0.19191919 0.2020202  0.21212121 0.22222222 0.23232323
 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929
 0.3030303  0.31313131 0.32323232 0.33333333 0.34343434 0.35353535
 0.36363636 0.37373737 0.38383838 0.39393939 0.4040404  0.41414141
 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747
 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354
 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.5959596
 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566
 0.66666667 0.67676768 0.68686869 0.6969697  0.70707071 0.71717172
 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778
 0.78787879 0.7979798  0.80808081 0.81818182 0.82828283 0.83838384
 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.8989899
 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596
 0.96969697 0.97979798 0.98989899 1.        ]
</pre></div>
</div>
</div>
</div>
<p>Per esempio, ponendo <span class="math notranslate nohighlight">\(\theta = 0.1\)</span> otteniamo il seguente valore dell’ordinata della funzione di verosimiglianza:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.1^{23} + (1-0.1)^7.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9.737168290200003e-18
</pre></div>
</div>
</div>
</div>
<p>Ponendo <span class="math notranslate nohighlight">\(\theta = 0.2\)</span> otteniamo il seguente valore dell’ordinata della funzione di verosimiglianza:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.2^{23} + (1-0.2)^7.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.581417234922211e-11
</pre></div>
</div>
</div>
</div>
<p>Se ripetiamo questo processo 100 volte, una volta per ciascuno dei valori <span class="math notranslate nohighlight">\(\theta\)</span> che abbiamo elencato sopra, otteniamo 100 coppie di punti <span class="math notranslate nohighlight">\(\theta\)</span> e <span class="math notranslate nohighlight">\(f(\theta)\)</span>. A tale fine, definiamo la seguente funzione.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">like</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span> <span class="o">*</span> <span class="n">theta</span><span class="o">**</span><span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">r</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>La curva che interpola i punti ottenuti è la funzione di verosimiglianza, come indicato dalla figura seguente.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">like</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">),</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Funzione di verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Valore della variabile casuale theta [0, 1]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Verosimiglianza&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../_images/00d8e85f13034870417288c086aedc1902951390bb6f7c693a4d37995db96c35.png"><img alt="../_images/00d8e85f13034870417288c086aedc1902951390bb6f7c693a4d37995db96c35.png" src="../_images/00d8e85f13034870417288c086aedc1902951390bb6f7c693a4d37995db96c35.png" style="width: 731px; height: 491px;" /></a>
</div>
</div>
</section>
</div>
<section id="interpretazione-della-funzione-di-verosimiglianza">
<h3>Interpretazione della Funzione di Verosimiglianza<a class="headerlink" href="#interpretazione-della-funzione-di-verosimiglianza" title="Link to this heading">#</a></h3>
<p>L’interpretazione della funzione di verosimiglianza ci permette di misurare l’adattamento dei vari valori di <span class="math notranslate nohighlight">\(\theta\)</span> ai dati. Il valore che massimizza la funzione indica la stima più plausibile di <span class="math notranslate nohighlight">\(\theta\)</span> dati i dati osservati. In termini pratici, se per esempio il valore che massimizza la verosimiglianza è <span class="math notranslate nohighlight">\(\theta = 0.767\)</span>, ciò suggerisce che la probabilità più plausibile di successo (o atteggiamento negativo) nella nostra popolazione di studio è del 76.7%.</p>
<p>La determinazione numerica di questo valore ottimale può avvenire attraverso tecniche computazionali, come l’identificazione del punto di massimo della funzione di verosimiglianza tramite metodi di ottimizzazione. L’uso di librerie statistiche e matematiche in linguaggi di programmazione come Python consente di effettuare queste analisi con precisione e efficienza, offrendo una stima accurata del parametro <span class="math notranslate nohighlight">\(\theta\)</span> che meglio si adatta ai dati osservati.</p>
<p>Questa metodologia, basata sull’uso della funzione di verosimiglianza, è cruciale per l’inferenza statistica, permettendo agli scienziati di stimare i parametri dei modelli statistici in modo informato e di valutare l’adeguatezza di tali modelli in rappresentanza dei dati reali.</p>
<p>In pratica, per identificare numericamente il valore ottimale di <span class="math notranslate nohighlight">\( \theta \)</span>, si può localizzare l’indice nel vettore dei valori di verosimiglianza dove questa raggiunge il suo picco. Metodi computazionali, come l’uso della funzione <code class="docutils literal notranslate"><span class="pre">argmax</span></code> in NumPy, possono automatizzare questo processo. Una volta individuato l’indice che massimizza la verosimiglianza, si può risalire al valore corrispondente di <span class="math notranslate nohighlight">\( \theta \)</span> nel vettore dei parametri, ottenendo così la stima di <span class="math notranslate nohighlight">\( \theta \)</span> che rende i dati osservati più plausibili.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">l</span> <span class="o">=</span> <span class="n">like</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">)</span>
<span class="n">l</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>76
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span><span class="p">[</span><span class="mi">76</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7676767676767677
</pre></div>
</div>
</div>
</div>
<p>È importante notare che, invece di utilizzare la funzione <code class="docutils literal notranslate"><span class="pre">like()</span></code> che abbiamo definito precedentemente per motivi didattici, è possibile ottenere lo stesso risultato utilizzando in modo equivalente la funzione <code class="docutils literal notranslate"><span class="pre">binom.pmf()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Funzione di verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Valore della variabile casuale theta [0, 1]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Verosimiglianza&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../_images/00d8e85f13034870417288c086aedc1902951390bb6f7c693a4d37995db96c35.png"><img alt="../_images/00d8e85f13034870417288c086aedc1902951390bb6f7c693a4d37995db96c35.png" src="../_images/00d8e85f13034870417288c086aedc1902951390bb6f7c693a4d37995db96c35.png" style="width: 731px; height: 491px;" /></a>
</div>
</div>
</section>
</section>
<section id="massima-verosimiglianza">
<h2>Massima verosimiglianza<a class="headerlink" href="#massima-verosimiglianza" title="Link to this heading">#</a></h2>
<p>Tra tutti i possibili valori <span class="math notranslate nohighlight">\(\theta\)</span> cerchiao il valore che massimizzi la probabilità dei dati osservati, ovvero, cerchiamo il valore <span class="math notranslate nohighlight">\(\theta\)</span> che corrisponde al massimo della funzione di verosimiglianza.</p>
<p>Parliamo di «minimizzazione» quando l “obiettivo è trovare il punto più basso in una valle (minimizzare) o di «massimizzazione», quando l’obiettivo è quello di trovare il punto più alto su una collina, a seconda della funzione. Nel caso della funzione di verosimiglianza, cerchiamo il punto in cui questa funzione raggiunge il suo valore massimo, ma poiché molti algoritmi sono progettati per trovare minimi, possiamo cercare il minimo del negativo della funzione di verosimiglianza, che corrisponde al massimo della funzione stessa.</p>
<p>La Strategia di Base</p>
<ul class="simple">
<li><p>Punto di Partenza: L’algoritmo inizia da un punto di partenza, che può essere scelto casualmente o basato su una qualche ipotesi ragionevole.</p></li>
<li><p>Esplorazione: L’algoritmo esplora la «superficie» della funzione, muovendosi in direzioni che sembrano portare verso il punto più basso (o più alto, se stiamo massimizzando). Questo è simile a sentire la pendenza del terreno intorno a noi per decidere in quale direzione camminare.</p></li>
<li><p>Aggiustamento: Man mano che procede, l’algoritmo aggiusta la sua traiettoria basandosi su ciò che ha «sentito» durante l’esplorazione. Se trova una discesa, continua in quella direzione; se incontra una salita, prova una direzione differente.</p></li>
<li><p>Convergenza: Il processo continua finché l’algoritmo non trova un punto in cui non ci sono più discese significative in nessuna direzione, suggerendo che ha trovato il punto più basso (o il punto più alto, se stiamo massimizzando) raggiungibile da quel percorso.</p></li>
</ul>
<p>Esistono diversi metodi che l’algoritmo può utilizzare per decidere come muoversi. Alcuni esempi includono:</p>
<ul class="simple">
<li><p>Discesa più ripida (Gradient Descent): Utilizza il gradiente (la direzione e la pendenza della collina) per decidere in quale direzione muoversi.</p></li>
<li><p>Newton-Raphson: Utilizza sia il gradiente sia la «curvatura» della funzione per fare passi più informati verso il minimo.</p></li>
<li><p>Algoritmi Genetici: Ispirati dall’evoluzione biologica, questi algoritmi «evolvono» una soluzione attraverso iterazioni che simulano la selezione naturale.</p></li>
</ul>
<p>In termini intuitivi, dunque, l’ottimizzazione è un processo metodico di esplorazione e aggiustamento basato su feedback immediato dalla funzione che stiamo cercando di ottimizzare, con l’obiettivo di trovare il punto di massimo o minimo valore.</p>
<p>Definiamo dunque il negativo della funzione di verosimiglianza per l’ottimizzazione (trovare il massimo della funzione):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">negative_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Calcolo del negativo della funzione di verosimiglianza</span>
    <span class="k">return</span> <span class="o">-</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">theta</span><span class="o">**</span><span class="n">y</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Utilizziamo ora <code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize</span></code> per trovare il valore di theta che massimizza la verosimiglianza. Bisogna specificare un valore iniziale per theta, qui assumiamo 0.5 come punto di partenza. I vincoli su theta sono che deve essere compreso tra 0 e 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">negative_likelihood</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span>
<span class="n">result</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.76666666])
</pre></div>
</div>
</div>
</div>
<section id="la-funzione-di-log-verosimiglianza">
<h3>La Funzione di Log-Verosimiglianza<a class="headerlink" href="#la-funzione-di-log-verosimiglianza" title="Link to this heading">#</a></h3>
<p>Proseguendo con il nostro approfondimento sull’analisi statistica mediante la funzione di verosimiglianza, ci spostiamo verso una sua trasformazione matematica spesso preferita dagli statistici: la funzione di log-verosimiglianza. Il passaggio alla log-verosimiglianza, definita come il logaritmo naturale della funzione di verosimiglianza:</p>
<div class="math notranslate nohighlight" id="equation-eq-loglike-definition">
<span class="eqno">(45)<a class="headerlink" href="#equation-eq-loglike-definition" title="Link to this equation">#</a></span>\[
\ell(\theta) = \log \mathcal{L}(\theta \mid y),
\]</div>
<p>non altera la posizione del massimo della funzione originale grazie alla proprietà di monotonicità del logaritmo. In termini pratici, ciò significa che il valore di <span class="math notranslate nohighlight">\(\theta\)</span> che massimizza la log-verosimiglianza, <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, è lo stesso che massimizza la verosimiglianza originale:</p>
<div class="math notranslate nohighlight">
\[
\hat{\theta} = \arg \max_{\theta \in \Theta} \ell(\theta) = \arg \max_{\theta \in \Theta} \mathcal{L}(\theta).
\]</div>
<p>Nell’analisi di un campione di osservazioni, l’uso della log-verosimiglianza semplifica il processo di massimizzazione, che può risultare complicato con la verosimiglianza tradizionale, soprattutto quando si gestiscono numeri molto piccoli. Questa semplificazione avviene perché la log-verosimiglianza trasforma il prodotto delle probabilità in una somma di logaritmi, rendendo il problema più semplice e numericamente stabile. L’espressione della log-verosimiglianza per un modello binomiale, ad esempio, si presenta come segue:</p>
<div class="math notranslate nohighlight">
\[
\ell(\theta \mid y) = \log(\theta^y (1 - \theta)^{n - y}) = y \log(\theta) + (n - y) \log(1 - \theta).
\]</div>
<p>Questa formulazione trasforma il prodotto delle probabilità di osservazioni indipendenti in una somma, facilitando notevolmente i calcoli, specialmente per dataset di grandi dimensioni o in presenza di calcoli complessi. La forma logaritmica è più gestibile e si presta meglio all’applicazione di tecniche di ottimizzazione numerica, grazie alla sua maggiore stabilità e alla riduzione di problemi come l’underflow, comuni quando si lavora con probabilità molto piccole.</p>
<p>Ritornando all’esempio della distribuzione binomiale, l’applicazione della log-verosimiglianza per il calcolo del parametro <span class="math notranslate nohighlight">\(\theta\)</span> che meglio si adatta ai dati osservati può essere eseguita con efficienza attraverso metodi computazionali. Per esempio, l’utilizzo di funzioni specifiche disponibili in pacchetti statistici, come <code class="docutils literal notranslate"><span class="pre">binom.logpmf()</span></code> in Python, permette di calcolare direttamente la log-verosimiglianza di un dato set di osservazioni per diversi valori di <span class="math notranslate nohighlight">\(\theta\)</span>. Questo approccio facilita la ricerca del valore di <span class="math notranslate nohighlight">\(\theta\)</span> che massimizza la log-verosimiglianza, fornendo una stima accurata e computazionalmente efficiente del parametro.</p>
<p>L’adozione della funzione di log-verosimiglianza, quindi, non solo consente di affrontare i limiti pratici legati alla manipolazione di piccole probabilità, ma offre anche un quadro concettuale chiaro per l’interpretazione della plausibilità dei parametri del modello alla luce dei dati osservati. Questa trasformazione logaritmica rappresenta un passaggio cruciale nell’analisi inferenziale, consentendo di stimare i parametri dei modelli statistici con maggiore precisione e affidabilità.</p>
<p>Per illustrare questo concetto, riprendiamo l’esempio precedente e applichiamo la funzione di log-verosimiglianza per identificare il valore di <span class="math notranslate nohighlight">\( \theta \)</span> che massimizza questa funzione. La rappresentazione grafica della funzione di log-verosimiglianza fornisce ulteriori intuizioni sul comportamento di questa funzione.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">23</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Funzione di log-verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Valore della variabile casuale theta [0, 1]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Log-verosimiglianza&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../_images/7ff5ade14a79a8834e08f85cb2d35b640ce544ce0ccd211ba5baf69c08b99ce3.png"><img alt="../_images/7ff5ade14a79a8834e08f85cb2d35b640ce544ce0ccd211ba5baf69c08b99ce3.png" src="../_images/7ff5ade14a79a8834e08f85cb2d35b640ce544ce0ccd211ba5baf69c08b99ce3.png" style="width: 731px; height: 491px;" /></a>
</div>
</div>
<p>Il risultato replica quello trovato in precedenza con la funzione di verosimiglianza.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ll</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
<span class="n">ll</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>76
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span><span class="p">[</span><span class="mi">76</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7676767676767677
</pre></div>
</div>
</div>
</div>
<p>Definizione della funzione del negativo della log-verosimiglianza con correzioni per evitare errori di dominio:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">corrected_negative_log_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Assicurarsi che theta sia all&#39;interno di un intervallo valido per evitare errori di logaritmo</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-10</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Utilizzo di <code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize</span></code> per trovare il valore di theta che massimizza la log-verosimiglianza:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result_log_likelihood_corrected</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span>
    <span class="n">corrected_negative_log_likelihood</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Il risultato ottimizzato per theta utilizzando la log-verosimiglianza corretta</span>
<span class="n">optimized_theta</span> <span class="o">=</span> <span class="n">result_log_likelihood_corrected</span><span class="o">.</span><span class="n">x</span>
<span class="n">optimized_theta</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.76666666])
</pre></div>
</div>
</div>
</div>
</section>
<section id="verosimiglianza-congiunta">
<h3>Verosimiglianza Congiunta<a class="headerlink" href="#verosimiglianza-congiunta" title="Link to this heading">#</a></h3>
<p>Proseguendo nella nostra esplorazione dell’inferenza statistica attraverso la funzione di verosimiglianza, ci concentriamo ora sul caso in cui abbiamo più osservazioni, tutte provenienti dalla stessa distribuzione binomiale e considerate indipendenti ed identicamente distribuite (IID). Tale scenario si presenta frequentemente nelle applicazioni pratiche, dove un insieme di <span class="math notranslate nohighlight">\(n\)</span> osservazioni <span class="math notranslate nohighlight">\(Y = [y_1, y_2, \ldots, y_n]\)</span> viene raccolto sotto le stesse condizioni sperimentali.</p>
<p>La chiave per analizzare queste osservazioni congiuntamente risiede nel calcolo della probabilità congiunta di <span class="math notranslate nohighlight">\(y_1, y_2, \ldots, y_n\)</span> data un’unica probabilità di successo <span class="math notranslate nohighlight">\(\theta\)</span> comune a tutte le prove. L’indipendenza delle osservazioni ci consente di esprimere questa probabilità congiunta come il prodotto delle probabilità individuali di ciascuna osservazione:</p>
<div class="math notranslate nohighlight">
\[
p(y_1, y_2, \ldots, y_n \mid \theta) = \prod_{i=1}^{n} p(y_i \mid \theta) = \prod_{i=1}^{n} \text{Binomiale}(y_i \mid \theta).
\]</div>
<p>La bellezza di questo approccio sta nel fatto che la verosimiglianza congiunta, che rappresenta la plausibilità complessiva di <span class="math notranslate nohighlight">\(\theta\)</span> data l’intera sequenza di osservazioni <span class="math notranslate nohighlight">\(Y\)</span>, è semplicemente il prodotto delle verosimiglianze individuali di ogni osservazione <span class="math notranslate nohighlight">\(y_i\)</span> rispetto a <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid Y) = \prod_{i=1}^{n} \mathcal{L}(\theta \mid y_i) = \prod_{i=1}^{n} p(y_i \mid \theta).
\]</div>
<p>Questa formulazione della verosimiglianza congiunta non solo evidenzia quanto bene il parametro <span class="math notranslate nohighlight">\(\theta\)</span> si adatta all’intero set di dati <span class="math notranslate nohighlight">\(Y\)</span>, ma offre anche una base metodologica solida per stimare <span class="math notranslate nohighlight">\(\theta\)</span>. Il parametro che massimizza la verosimiglianza congiunta, noto come stimatore di massima verosimiglianza (MLE) di <span class="math notranslate nohighlight">\(\theta\)</span>, è quello che si ritiene essere il più plausibile data l’osservazione dei dati.</p>
<p>Quando abbiamo più gruppi di osservazioni bernoulliane indipendenti ed identicamente distribuite (iid), la funzione di log-verosimiglianza congiunta per tutti i gruppi può essere espressa come la somma delle log-verosimiglianze di ciascun gruppo. Ciò è dovuto alla proprietà che il logaritmo del prodotto è la somma dei logaritmi.</p>
<p>Supponiamo di avere i seguenti dati per 4 gruppi di osservazioni:</p>
<ul class="simple">
<li><p>Gruppo 1: 30 prove con 23 successi</p></li>
<li><p>Gruppo 2: 28 prove con 21 successi</p></li>
<li><p>Gruppo 3: 40 prove con 31 successi</p></li>
<li><p>Gruppo 4: 36 prove con 29 successi</p></li>
</ul>
<p>La funzione di log-verosimiglianza congiunta per questi dati, assumendo una singola probabilità di successo <span class="math notranslate nohighlight">\(\theta\)</span> per tutti i gruppi, è data da:</p>
<div class="math notranslate nohighlight">
\[
\log L(\theta) = \sum_{i=1}^{4} \left[ y_i \log(\theta) + (n_i - y_i) \log(1 - \theta) \right],
\]</div>
<p>dove <span class="math notranslate nohighlight">\(n_i\)</span> e <span class="math notranslate nohighlight">\(y_i\)</span> sono rispettivamente il numero di prove e il numero di successi nel <span class="math notranslate nohighlight">\(i\)</span>-esimo gruppo.</p>
<p>Per trovare il valore di <span class="math notranslate nohighlight">\(\theta\)</span> che massimizza questa funzione di log-verosimiglianza, possiamo usare il metodo di ottimizzazione <code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize</span></code>, come abbiamo fatto in precedenza. Definiamo prima la funzione di log-verosimiglianza congiunta (usiamo <code class="docutils literal notranslate"><span class="pre">np.clip</span></code> per evitare errori):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_verosimiglianza_congiunta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">dati</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-10</span><span class="p">)</span>  <span class="c1"># Evita valori esattamente 0 o 1</span>
    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dati</span><span class="p">:</span>
        <span class="n">log_likelihood</span> <span class="o">+=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">log_likelihood</span>  <span class="c1"># Restituisce il negativo per l&#39;ottimizzazione</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dati dei gruppi: (prove, successi)</span>
<span class="n">dati_gruppi</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">23</span><span class="p">),</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">29</span><span class="p">),</span> <span class="p">(</span><span class="mi">36</span><span class="p">,</span> <span class="mi">29</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dati_gruppi</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(30, 23), (28, 20), (40, 29), (36, 29)]
</pre></div>
</div>
</div>
</div>
<p>Ottimizzazione con la funzione <code class="docutils literal notranslate"><span class="pre">log_verosimiglianza_congiunta</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span>
    <span class="n">log_verosimiglianza_congiunta</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">dati_gruppi</span><span class="p">,),</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="p">)</span>

<span class="c1"># Il risultato ottimizzato per theta con la funzione corretta</span>
<span class="n">result</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.75373134])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Intervallo di valori di theta da esplorare</span>
<span class="n">theta_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Calcolo dei valori di log-verosimiglianza per ogni theta</span>
<span class="n">log_likelihood_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_verosimiglianza_congiunta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">dati_gruppi</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">theta_values</span><span class="p">]</span>

<span class="c1"># Creazione del grafico</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_values</span><span class="p">,</span> <span class="n">log_likelihood_values</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Log-verosimiglianza&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Theta&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Log-verosimiglianza negativa&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Funzione di Log-verosimiglianza&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../_images/8172b0016ac0aab66eb601b529869616c3697c1c649df19a6251ec294245716e.png"><img alt="../_images/8172b0016ac0aab66eb601b529869616c3697c1c649df19a6251ec294245716e.png" src="../_images/8172b0016ac0aab66eb601b529869616c3697c1c649df19a6251ec294245716e.png" style="width: 1011px; height: 611px;" /></a>
</div>
</div>
</section>
</section>
<section id="la-verosimiglianza-marginale">
<h2>La Verosimiglianza Marginale<a class="headerlink" href="#la-verosimiglianza-marginale" title="Link to this heading">#</a></h2>
<p>Avanzando nella nostra discussione sulla verosimiglianza, approfondiamo ora un passaggio cruciale nell’applicazione della teoria bayesiana: il concetto di verosimiglianza marginale. Questo approccio si rivela essenziale quando affrontiamo situazioni in cui il parametro di interesse, <span class="math notranslate nohighlight">\(\theta\)</span>, non è definito da un valore singolo e fisso, ma è invece descritto da una distribuzione di probabilità che riflette la nostra incertezza o variabilità su di esso.</p>
<p>In contesti pratici, non è raro incontrare scenari in cui <span class="math notranslate nohighlight">\(\theta\)</span> può assumere una gamma di valori, ciascuno con una probabilità associata, piuttosto che un valore deterministico. L’integrazione del parametro <span class="math notranslate nohighlight">\(\theta\)</span> permette di calcolare la probabilità complessiva (o verosimiglianza) di osservare un determinato risultato dati tutti i possibili valori di <span class="math notranslate nohighlight">\(\theta\)</span>, piuttosto che appoggiarsi a un’analisi basata su un singolo valore di <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Consideriamo, per esempio, una situazione in cui stiamo osservando una sequenza di prove binomiali, con un risultato specifico di interesse (ad esempio, <span class="math notranslate nohighlight">\(k=7\)</span> successi su <span class="math notranslate nohighlight">\(n=10\)</span> prove). Se <span class="math notranslate nohighlight">\(\theta\)</span> rappresenta la probabilità di successo in ciascuna prova e può assumere un insieme discreto di valori (per esempio, 0.1, 0.5, e 0.9) con probabilità uniforme, la verosimiglianza di osservare il nostro risultato specifico può essere espressa come:</p>
<div class="math notranslate nohighlight">
\[p(k=7, n=10) = \sum_{\theta \in \{0.1, 0.5, 0.9\}} \binom{10}{7} \theta^7 (1-\theta)^3 p(\theta),\]</div>
<p>dove <span class="math notranslate nohighlight">\(p(\theta)\)</span> rappresenta la probabilità associata a ciascun possibile valore di <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Tuttavia, in molte applicazioni reali, <span class="math notranslate nohighlight">\(\theta\)</span> può variare continuamente all’interno di un intervallo, come tra 0 e 1 per una distribuzione binomiale. In questi casi, il calcolo della verosimiglianza marginale richiede l’utilizzo dell’integrazione su tutto lo spazio dei valori possibili di <span class="math notranslate nohighlight">\(\theta\)</span>, riflettendo la gamma continua di possibili probabilità di successo. La formula si estende quindi a:</p>
<div class="math notranslate nohighlight">
\[p(k=7, n=10) = \int_{0}^{1} \binom{10}{7} \theta^7 (1-\theta)^3 p(\theta) d\theta,\]</div>
<p>dove <span class="math notranslate nohighlight">\(p(\theta) d\theta\)</span> rappresenta la densità di probabilità di <span class="math notranslate nohighlight">\(\theta\)</span> su un intervallo infinitesimale, e l’integrale copre tutti i possibili valori di <span class="math notranslate nohighlight">\(\theta\)</span> da 0 a 1. Implementare questo calcolo nell’ambito di uno spazio continuo richiede l’utilizzo di tecniche di integrazione.</p>
<p>Vediamo come sia possibile eseguire questo calcolo in Python, utilizzando la libreria <code class="docutils literal notranslate"><span class="pre">scipy</span></code> per l’integrazione su uno spazio continuo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Definire la funzione di verosimiglianza</span>
<span class="k">def</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">)</span>

<span class="c1"># Calcolare la verosimiglianza marginale integrando su θ</span>
<span class="n">marginal_likelihood</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">quad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;La verosimiglianza marginale è:&quot;</span><span class="p">,</span> <span class="n">marginal_likelihood</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La verosimiglianza marginale è: 0.09090909090909094
</pre></div>
</div>
</div>
</div>
<p>Questo codice esegue l’integrazione della funzione di verosimiglianza binomiale su tutti i possibili valori di θ (da 0 a 1), fornendo così la verosimiglianza marginale per il nostro esempio. Questo processo ci permette di considerare l’incertezza su θ, offrendo una visione completa della verosimiglianza dell’evento osservato senza fissare θ a un singolo valore.</p>
<p>Numericamente, nell’esempio della verosimiglianza basata su una distribuzione binomiale precedente, la verosimiglianza marginale è effettivamente interpretata come l’area sottesa dalla funzione di verosimiglianza, calcolata integrandola su tutto l’intervallo dei possibili valori di <span class="math notranslate nohighlight">\(\theta\)</span> (da 0 a 1, nel contesto di probabilità). Questa operazione di integrazione fornisce un valore che quantifica l’area sotto la curva della funzione di verosimiglianza. Importante sottolineare, questo valore non corrisponde alla probabilità dei dati dati i parametri, dato che la verosimiglianza non è una densità di probabilità sui parametri. Piuttosto, esso misura in che misura l’intero modello, considerando tutti i possibili valori del parametro, è in grado di spiegare i dati osservati.</p>
<p>La vera importanza della verosimiglianza marginale emerge nel contesto dell’inferenza bayesiana: essa agisce come fattore di normalizzazione nella formula di Bayes. Nello specifico, la verosimiglianza marginale normalizza la funzione risultante dal prodotto tra la verosimiglianza e la distribuzione a priori dei parametri (il numeratore nella formula di Bayes), garantendo che il risultato sia una distribuzione di probabilità valida sullo spazio dei parametri. In altre parole, la verosimiglianza marginale assicura che l’area sotto la curva della distribuzione posteriore sia esattamente 1, rendendola così una vera distribuzione di probabilità.</p>
</section>
<section id="modello-gaussiano-e-verosimiglianza">
<h2>Modello Gaussiano e Verosimiglianza<a class="headerlink" href="#modello-gaussiano-e-verosimiglianza" title="Link to this heading">#</a></h2>
<p>Ampliamo ora la nostra analisi al caso della distribuzione gaussiana. Inizieremo con la verosimiglianza associata a una singola osservazione <span class="math notranslate nohighlight">\( Y \)</span>, per poi estendere la discussione a un insieme di osservazioni gaussiane indipendenti e identicamente distribuite (IID).</p>
<section id="caso-di-una-singola-osservazione">
<h3>Caso di una Singola Osservazione<a class="headerlink" href="#caso-di-una-singola-osservazione" title="Link to this heading">#</a></h3>
<p>Iniziamo esaminiamo il caso di una singola osservazione. Quale esempio, prendiamo in considerazione la situazione in cui una variabile casuale rappresenta il Quoziente d’Intelligenza (QI) di un individuo. Se consideriamo la distribuzione del QI come gaussiana, possiamo esprimere la funzione di verosimiglianza per un singolo valore osservato di QI tramite la formula della distribuzione gaussiana, che misura la probabilità di osservare quel particolare valore di QI dato un insieme di parametri specifici, <span class="math notranslate nohighlight">\(\mu\)</span> (la media) e <span class="math notranslate nohighlight">\(\sigma\)</span> (la deviazione standard). La verosimiglianza offre quindi un modo per quantificare quanto bene i parametri <span class="math notranslate nohighlight">\(\mu\)</span> e <span class="math notranslate nohighlight">\(\sigma\)</span> si accordano con il valore osservato di QI.</p>
<p>Supponiamo che il QI osservato sia 114 e, per semplicità, assumiamo che la deviazione standard <span class="math notranslate nohighlight">\(\sigma\)</span> sia conosciuta e pari a 15. Vogliamo esaminare un’ampia gamma di possibili valori per la media <span class="math notranslate nohighlight">\(\mu\)</span>, diciamo tra 70 e 160, e valutare quale di questi valori rende più plausibile l’osservazione fatta Definiamo quindi un insieme di 1000 valori per <span class="math notranslate nohighlight">\(\mu\)</span> da esplorare:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">70.0</span><span class="p">,</span> <span class="mf">160.0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">114</span>
</pre></div>
</div>
</div>
</div>
<p>La nostra analisi consiste nell’applicare la funzione di densità di probabilità gaussiana a ciascuno di questi 1000 valori di <span class="math notranslate nohighlight">\(\mu\)</span>, mantenendo fisso il valore osservato di QI, <span class="math notranslate nohighlight">\(y=114\)</span>, e la deviazione standard, <span class="math notranslate nohighlight">\(\sigma=15\)</span>. In questo modo, possiamo costruire la funzione di verosimiglianza che esprime la plausibilità di ciascun valore di <span class="math notranslate nohighlight">\(\mu\)</span> alla luce del QI osservato.</p>
<p>Il calcolo specifico della densità di probabilità per ogni valore di <span class="math notranslate nohighlight">\(\mu\)</span> può essere eseguito con la funzione <code class="docutils literal notranslate"><span class="pre">norm.pdf</span></code> di <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code>, che accetta il valore osservato <span class="math notranslate nohighlight">\(y\)</span>, un array di medie (i nostri valori di <span class="math notranslate nohighlight">\(\mu\)</span>) e la deviazione standard <span class="math notranslate nohighlight">\(\sigma\)</span>. Per un singolo valore <code class="docutils literal notranslate"><span class="pre">mu</span></code> = 70, otteniamo</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.00036007041207962535
</pre></div>
</div>
</div>
</div>
<p>Per il valore <code class="docutils literal notranslate"><span class="pre">mu</span></code> = 70.05 otteniamo</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mf">70.05</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.00036360634900376967
</pre></div>
</div>
</div>
</div>
<p>e così via. Se usiamo utti i 1000 valori possibili di <code class="docutils literal notranslate"><span class="pre">mu</span></code>, otteniamo un vettore di 1000 risultati:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f_mu</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Questo passaggio ci fornisce un array di valori che rappresentano la verosimiglianza di ciascun valore di <span class="math notranslate nohighlight">\(\mu\)</span> data l’osservazione <span class="math notranslate nohighlight">\(y\)</span>. Tracciando questi valori <code class="docutils literal notranslate"><span class="pre">f_mu</span></code> in funzione di <span class="math notranslate nohighlight">\(\mu\)</span>, otteniamo una curva di verosimiglianza che illustra visivamente quanto bene ciascun valore di <span class="math notranslate nohighlight">\(\mu\)</span> si adatta al dato osservato <code class="docutils literal notranslate"><span class="pre">y</span></code> = 114:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">f_mu</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Funzione di verosimiglianza per QI = 114&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Valore di mu [70, 160]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">70</span><span class="p">,</span> <span class="mi">160</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../_images/ad7988980eaed9440cd11b600ed9ebe658f66f28616da78b9d87dbf021aa3f06.png"><img alt="../_images/ad7988980eaed9440cd11b600ed9ebe658f66f28616da78b9d87dbf021aa3f06.png" src="../_images/ad7988980eaed9440cd11b600ed9ebe658f66f28616da78b9d87dbf021aa3f06.png" style="width: 731px; height: 491px;" /></a>
</div>
</div>
<p>Abbiamo dunque proceduto come nel caso della distribuzione binomiale esaminata in precedenza. Abbiamo utilizzato la formula</p>
<div class="math notranslate nohighlight">
\[
f(x | \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right),
\]</div>
<p>tenendo costante il valore <span class="math notranslate nohighlight">\(x\)</span> = 114 e considerando noto <span class="math notranslate nohighlight">\(\sigma\)</span> = 15, e abbiamo applicato la formula 1000 volte facendo variare <code class="docutils literal notranslate"><span class="pre">mu</span></code> ogni volta utilizziando ciascuno dei valori definiti con <code class="docutils literal notranslate"><span class="pre">np.linspace(70.0,</span> <span class="pre">160.0,</span> <span class="pre">num=1000)</span></code>.</p>
<p>La moda della distribuzione, si trova con</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimal_mu</span> <span class="o">=</span> <span class="n">mu</span><span class="p">[</span><span class="n">f_mu</span><span class="o">.</span><span class="n">argmax</span><span class="p">()]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">optimal_mu</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>113.96396396396396
</pre></div>
</div>
</div>
</div>
<p>In questo esempio, otteniamo il valore <span class="math notranslate nohighlight">\(\mu\)</span> = 113.96 che massimizza la verosimiglianza.</p>
<p>Per calcolare il massimo della log-verosimiglianza per una distribuzione Gaussiana usando la funzione <code class="docutils literal notranslate"><span class="pre">optimize()</span></code> di SciPy, possiamo seguire questi passi. Partiamo dalla formula della densità di probabilità della distribuzione gaussiana per una singola osservazione <span class="math notranslate nohighlight">\(y\)</span>, con media <span class="math notranslate nohighlight">\(\mu\)</span> e deviazione standard <span class="math notranslate nohighlight">\(\sigma\)</span>. La formula è:</p>
<div class="math notranslate nohighlight">
\[
f(y \mid \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right)
\]</div>
<p>Poiché abbiamo una singola osservazione <span class="math notranslate nohighlight">\(y\)</span>, la funzione di verosimiglianza coincide con la funzione di densità di probabilità. Quindi, prendiamo il logaritmo naturale di entrambi i lati della equazione della densità di probabilità gaussiana per ottenere la log-verosimiglianza:</p>
<div class="math notranslate nohighlight">
\[
\log f(y \mid \mu, \sigma) = \log \left( \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right) \right)
\]</div>
<p>Applichiamo le proprietà dei logaritmi. Ricordiamo che:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\log(ab) = \log(a) + \log(b)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\log\left(\frac{1}{a}\right) = -\log(a)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\log(e^x) = x\)</span></p></li>
</ul>
<p>Quindi, possiamo scrivere:</p>
<div class="math notranslate nohighlight">
\[
\log f(y \mid \mu, \sigma) = \log\left(\frac{1}{\sigma \sqrt{2\pi}}\right) + \log\left(\exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right)\right)
\]</div>
<div class="math notranslate nohighlight">
\[
= -\log(\sigma \sqrt{2\pi}) -\frac{(y - \mu)^2}{2\sigma^2}.
\]</div>
<p>Ricordando che <span class="math notranslate nohighlight">\(\log(ab) = \log(a) + \log(b)\)</span>, possiamo scrivere <span class="math notranslate nohighlight">\(\log(\sigma \sqrt{2\pi})\)</span> come la somma di due logaritmi:</p>
<div class="math notranslate nohighlight">
\[
-\log(\sigma \sqrt{2\pi}) = -\log(\sigma) - \log(\sqrt{2\pi}).
\]</div>
<p>E dato che <span class="math notranslate nohighlight">\(\log(\sqrt{2\pi}) = \frac{1}{2}\log(2\pi)\)</span>, possiamo sostituire per ottenere:</p>
<div class="math notranslate nohighlight">
\[
-\log(\sigma) - \frac{1}{2}\log(2\pi).
\]</div>
<p>Combinando tutto, otteniamo:</p>
<div class="math notranslate nohighlight">
\[
\log L(\mu; y, \sigma) = -\frac{1}{2} \log(2 \pi) - \log(\sigma) - \frac{(y - \mu)^2}{2 \sigma^2}.
\]</div>
<p>Questa è la trasformata logaritmica della funzione di densità di probabilità gaussiana per una singola osservazione, che rappresenta la log-verosimiglianza di osservare <span class="math notranslate nohighlight">\(y\)</span> dato <span class="math notranslate nohighlight">\(\mu\)</span> e <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p>Vogliamo trovare il valore di <span class="math notranslate nohighlight">\(\mu\)</span> che massimizza questa funzione di log-verosimiglianza. Siccome <code class="docutils literal notranslate"><span class="pre">optimize()</span></code> di SciPy minimizza una funzione, possiamo passare il negativo della log-verosimiglianza per trovare il massimo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dati osservati</span>
<span class="n">y_obs</span> <span class="o">=</span> <span class="mi">114</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mi">15</span>

<span class="c1"># Definizione della funzione negativa della log-verosimiglianza</span>
<span class="k">def</span> <span class="nf">negative_log_likelihood</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span> <span class="o">+</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Ottimizzazione per trovare il valore di mu che massimizza la log-verosimiglianza</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">negative_log_likelihood</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">y_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>

<span class="c1"># Il risultato ottimizzato per mu</span>
<span class="n">result</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([113.99997648])
</pre></div>
</div>
</div>
</div>
<p>Il valore di <span class="math notranslate nohighlight">\(\mu\)</span> che massimizza la log-verosimiglianza per una distribuzione Gaussiana con <span class="math notranslate nohighlight">\(y = 114\)</span> e <span class="math notranslate nohighlight">\(\sigma = 15\)</span> è circa <span class="math notranslate nohighlight">\(114\)</span>. Questo risultato dimostra che, nel caso di una distribuzione Gaussiana con una singola osservazione e deviazione standard nota, il massimo della log-verosimiglianza si ottiene quando la media stimata <span class="math notranslate nohighlight">\(\mu\)</span> è molto vicina al valore osservato <span class="math notranslate nohighlight">\(y\)</span>.</p>
</section>
<section id="campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana">
<h3>Campione indipendente di osservazioni da una distribuzione gaussiana<a class="headerlink" href="#campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana" title="Link to this heading">#</a></h3>
<p>Passiamo ora all’esame di un contesto più complesso: quello di un campione composto da <span class="math notranslate nohighlight">\( n \)</span> osservazioni indipendenti, tutte provenienti da una distribuzione gaussiana. Consideriamo questo insieme di osservazioni come realizzazioni indipendenti ed identicamente distribuite (i.i.d.) di una variabile casuale <span class="math notranslate nohighlight">\( X \)</span>, che segue una distribuzione normale con media <span class="math notranslate nohighlight">\( \mu \)</span> e deviazione standard <span class="math notranslate nohighlight">\( \sigma \)</span>, entrambi parametri sconosciuti. Denotiamo questa situazione con la notazione <span class="math notranslate nohighlight">\( X \sim N(\mu, \sigma^2) \)</span>.</p>
<p>In presenza di osservazioni i.i.d., la densità di probabilità congiunta del campione è il prodotto delle funzioni di densità per ogni singola osservazione. Matematicamente, ciò si esprime attraverso l’equazione:</p>
<div class="math notranslate nohighlight">
\[ p(y_1, y_2, \ldots, y_n | \mu, \sigma) = \prod_{i=1}^{n} p(y_i | \mu, \sigma), \]</div>
<p>dove <span class="math notranslate nohighlight">\( p(y_i | \mu, \sigma) \)</span> indica la funzione di densità gaussiana per l’osservazione <span class="math notranslate nohighlight">\( y_i \)</span>, parametrizzata da <span class="math notranslate nohighlight">\( \mu \)</span> e <span class="math notranslate nohighlight">\( \sigma \)</span>.</p>
<p>Se manteniamo i dati osservati come costanti, ciò che cambia in questa equazione quando variamo <span class="math notranslate nohighlight">\( \mu \)</span> e <span class="math notranslate nohighlight">\( \sigma \)</span> sono le probabilità associate ad ogni configurazione dei parametri, portandoci così alla funzione di verosimiglianza congiunta per il campione.</p>
<div class="exercise admonition" id="ex-likelihood-bdi">

<p class="admonition-title"><span class="caption-number">Exercise 141 </span></p>
<section id="exercise-content">
<p>Consideriamo, per illustrare questa dinamica, il caso di uno studio clinico che misura i punteggi del Beck Depression Inventory II (BDI-II) su trenta partecipanti. Supponiamo che questi punteggi seguano una distribuzione normale. Dati i punteggi BDI-II per i trenta partecipanti, il nostro obiettivo è costruire una funzione di verosimiglianza per questi dati, assumendo che la deviazione standard <span class="math notranslate nohighlight">\( \sigma \)</span> sia nota e pari alla deviazione standard campionaria di 6.50.</p>
<p>Per la totalità del campione, la densità di probabilità congiunta diventa quindi il prodotto delle densità per ogni osservazione. Di conseguenza, la funzione di verosimiglianza per il campione intero è rappresentata dal prodotto delle densità di probabilità di tutte le osservazioni.</p>
<p>In questo contesto, ogni possibile valore di <span class="math notranslate nohighlight">\( \mu \)</span> viene valutato in termini di verosimiglianza. Per esemplificare, consideriamo un range di 1000 valori per <span class="math notranslate nohighlight">\( \mu \)</span> e calcoliamo la funzione di verosimiglianza per ognuno di questi. Per rendere più gestibili i calcoli, utilizziamo il logaritmo della funzione di verosimiglianza.</p>
<p>Definendo una funzione <code class="docutils literal notranslate"><span class="pre">log_likelihood</span></code> in Python che accetta i punteggi BDI-II <span class="math notranslate nohighlight">\( y \)</span>, un valore medio <span class="math notranslate nohighlight">\( \mu \)</span>, e imposta <span class="math notranslate nohighlight">\( \sigma \)</span> al valore noto, possiamo calcolare la log-verosimiglianza per un’ampia gamma di valori di <span class="math notranslate nohighlight">\( \mu \)</span> entro un intervallo specifico. Ciò ci permette di visualizzare la credibilità relativa di ciascun valore di <span class="math notranslate nohighlight">\( \mu \)</span> alla luce dei dati osservati.</p>
<p>Infine, il valore di <span class="math notranslate nohighlight">\( \mu \)</span> che massimizza la funzione di log-verosimiglianza corrisponde alla stima di massima verosimiglianza di <span class="math notranslate nohighlight">\( \mu \)</span> data la distribuzione dei punteggi BDI-II nel campione. Questo valore, nel nostro esempio, coincide con la media campionaria dei punteggi BDI-II, offrendo una stima concorde con l’intuizione che la media del campione sia un buon rappresentante del parametro <span class="math notranslate nohighlight">\( \mu \)</span> in una distribuzione normale.</p>
<p>I dati sono:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mi">26</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">39</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span>
    <span class="mi">28</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Il nostro scopo è sviluppare una funzione di verosimiglianza utilizzando le 30 osservazioni indicate sopra. Basandoci su studi precedenti, ipotizziamo che questi punteggi seguano una distribuzione normale. Assumiamo inoltre che la deviazione standard <span class="math notranslate nohighlight">\( \sigma \)</span> sia nota e corrisponda a quella osservata nel campione, ossia 6.50.</p>
<p>Per la prima osservazione del campione, dove <span class="math notranslate nohighlight">\( y_1 = 26 \)</span>, la funzione di densità di probabilità si esprime come:</p>
<div class="math notranslate nohighlight">
\[
f(26 \,|\, \mu, \sigma = 6.50) = \frac{1}{6.50\sqrt{2\pi}} \exp \left( -\frac{(26 - \mu)^2}{2 \cdot 6.50^2} \right).
\]</div>
<p>Estendendo questo calcolo all’intero campione, la funzione di densità di probabilità congiunta si ottiene come il prodotto delle densità di tutte le osservazioni individuali:</p>
<div class="math notranslate nohighlight">
\[
f(y \,|\, \mu, \sigma = 6.50) = \prod_{i=1}^{n} f(y_i \,|\, \mu, \sigma = 6.50).
\]</div>
<p>Di conseguenza, la funzione di verosimiglianza, indicata con <span class="math notranslate nohighlight">\( \mathcal{L}(\mu, \sigma = 6.50 \,|\, y) \)</span>, si determina moltiplicando insieme le densità di probabilità di tutte le osservazioni nel campione:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{L}(\mu, \sigma=6.50 \,|\, y) &amp;= \prod_{i=1}^{30} \frac{1}{6.50\sqrt{2\pi}} \exp \left( -\frac{(y_i - \mu)^2}{2 \cdot 6.50^2} \right) \\
&amp;= \left( \frac{1}{6.50\sqrt{2\pi}} \right)^{30} \exp\left( -\sum_{i=1}^{30} \frac{(y_i - \mu)^2}{2 \cdot 6.50^2} \right).
\end{aligned}
\end{split}\]</div>
<p>In questa formula, <span class="math notranslate nohighlight">\( \mu \)</span> rappresenta il parametro di interesse, la media della distribuzione, la cui stima massimizza la funzione di verosimiglianza. Se si considerano 1000 valori differenti per <span class="math notranslate nohighlight">\( \mu \)</span>, dovremmo calcolare la funzione di verosimiglianza per ciascuno di questi valori.</p>
<p>Per rendere i calcoli più gestibili, è consigliabile utilizzare il logaritmo della funzione di verosimiglianza. In Python, possiamo definire una funzione <code class="docutils literal notranslate"><span class="pre">log_likelihood()</span></code> che accetta come argomenti <code class="docutils literal notranslate"><span class="pre">y</span></code>, <code class="docutils literal notranslate"><span class="pre">mu</span></code> e <code class="docutils literal notranslate"><span class="pre">sigma</span> <span class="pre">=</span> <span class="pre">true_sigma</span></code>. Per semplificare, impostiamo <code class="docutils literal notranslate"><span class="pre">true_sigma</span></code> uguale alla deviazione standard osservata nel campione.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">true_sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">true_sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6.495810615739622
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">true_sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">true_sigma</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Consideriamo, ad esempio, il valore <span class="math notranslate nohighlight">\(\mu_0 = \bar{y}\)</span>, ovvero</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bar_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bar_y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>30.933333333333334
</pre></div>
</div>
</div>
</div>
<p>L’ordinata della funzione di log-verosimiglianza in corrispondenza di <span class="math notranslate nohighlight">\(\mu = 30.93\)</span> è</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mf">30.93</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">true_sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-98.70288339960591
</pre></div>
</div>
</div>
</div>
<p>Troviamo ora i valori della log-verosimiglianza per ciascuno dei 1000 valori <span class="math notranslate nohighlight">\(\mu\)</span> nell’intervallo <span class="math notranslate nohighlight">\([\bar{y} - 2 \sigma, \bar{y} + 2 \sigma]\)</span>. Iniziamo a definire il vettore <code class="docutils literal notranslate"><span class="pre">mu</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Troviamo il valore dell’ordinata della funzione di log-verosimiglianza in corrispondenza di ciascuno dei 1000 valori <code class="docutils literal notranslate"><span class="pre">mu</span></code> che abbiamo definito.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ll</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mu_val</span><span class="p">,</span> <span class="n">true_sigma</span><span class="p">)</span> <span class="k">for</span> <span class="n">mu_val</span> <span class="ow">in</span> <span class="n">mu</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Nel caso di un solo parametro sconosciuto (nel caso presente, <span class="math notranslate nohighlight">\(\mu\)</span>) è possibile rappresentare la log-verosimiglianza con una curva che interpola i punti (<code class="docutils literal notranslate"><span class="pre">mu</span></code>, <code class="docutils literal notranslate"><span class="pre">ll</span></code>). Tale funzione descrive la <em>credibilità relativa</em> che può essere attribuita ai valori del parametro <span class="math notranslate nohighlight">\(\mu\)</span> alla luce dei dati osservati.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">ll</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Funzione di log-verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Valore della variabile casuale mu&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Log-verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../_images/d40930eeddbefd3498f7707fedc75103118e48901e3f7b5b93f1c3d96c4a0ce7.png"><img alt="../_images/d40930eeddbefd3498f7707fedc75103118e48901e3f7b5b93f1c3d96c4a0ce7.png" src="../_images/d40930eeddbefd3498f7707fedc75103118e48901e3f7b5b93f1c3d96c4a0ce7.png" style="width: 731px; height: 491px;" /></a>
</div>
</div>
<p>Il valore <span class="math notranslate nohighlight">\(\mu\)</span> più credibile corrisponde al massimo della funzione di log-verosimiglinza e viene detto <em>stima di massima verosimiglianza</em>.</p>
<p>Il massimo della funzione di log-verosimiglianza, ovvero 30.93 per l’esempio in discussione, è identico alla media dei dati campionari.</p>
<p>Per applicare lo stesso approccio usato precedentemente con <code class="docutils literal notranslate"><span class="pre">optimize</span></code> ad un campione di dati, anziché a una singola osservazione, possiamo modificare la funzione di log-verosimiglianza per prendere in considerazione tutte le osservazioni nel campione. La log-verosimiglianza per un campione da una distribuzione Gaussiana, dove ogni osservazione <span class="math notranslate nohighlight">\(y_i\)</span> ha la stessa media <span class="math notranslate nohighlight">\(\mu\)</span> e deviazione standard <span class="math notranslate nohighlight">\(\sigma\)</span>, è la somma delle log-verosimiglianze di ogni osservazione individuale.</p>
<p>La formula modificata per il campione sarà:</p>
<div class="math notranslate nohighlight">
\[
\log L(\mu; y, \sigma) = \sum_{i=1}^{n} \left[ -\frac{1}{2} \log(2 \pi) - \log(\sigma) - \frac{(y_i - \mu)^2}{2 \sigma^2} \right],
\]</div>
<p>dove <span class="math notranslate nohighlight">\(y\)</span> è l’array delle osservazioni e <span class="math notranslate nohighlight">\(n\)</span> è il numero di osservazioni nel campione.</p>
<p>Poiché, per semplicità, assumiamo <span class="math notranslate nohighlight">\(\sigma\)</span> come la deviazione standard del campione, prima calcoleremo <span class="math notranslate nohighlight">\(\sigma\)</span> dal campione fornito e poi useremo quel valore per l’ottimizzazione della log-verosimiglianza, cercando il valore di <span class="math notranslate nohighlight">\(\mu\)</span> che la massimizza.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calcolo della deviazione standard del campione</span>
<span class="n">sigma_sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Definizione della funzione negativa della log-verosimiglianza per il campione</span>
<span class="k">def</span> <span class="nf">negative_log_likelihood_sample</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">n</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Ottimizzazione per trovare il valore di mu che massimizza la log-verosimiglianza per il campione</span>
<span class="n">result_sample</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">negative_log_likelihood_sample</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sigma_sample</span><span class="p">))</span>

<span class="c1"># Il risultato ottimizzato per mu</span>
<span class="n">result_sample</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([30.93333333])
</pre></div>
</div>
</div>
</div>
<p>Il valore di <span class="math notranslate nohighlight">\(\mu\)</span> che massimizza la log-verosimiglianza per il campione di dati fornito, assumendo noto il valore di <span class="math notranslate nohighlight">\(\sigma\)</span> (la deviazione standard del campione), è circa <span class="math notranslate nohighlight">\(30.93\)</span>. Questo rappresenta la stima ottimale per la media della distribuzione Gaussiana che meglio si adatta al campione di dati dato.</p>
</section>
</div>
</section>
<section id="la-stima-di-massima-verosimiglianza-per-mu">
<h3>La Stima di Massima Verosimiglianza per <span class="math notranslate nohighlight">\( \mu \)</span><a class="headerlink" href="#la-stima-di-massima-verosimiglianza-per-mu" title="Link to this heading">#</a></h3>
<p>Per determinare il valore di <span class="math notranslate nohighlight">\( \mu \)</span> che massimizza la funzione di log-verosimiglianza, procediamo calcolando la sua derivata parziale rispetto a <span class="math notranslate nohighlight">\( \mu \)</span> e impostando il risultato uguale a zero:</p>
<ol class="arabic">
<li><p>Partiamo dalla funzione di log-verosimiglianza, che è data da:</p>
<div class="math notranslate nohighlight">
\[
   \ell = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2.
   \]</div>
</li>
<li><p>Calcoliamo la derivata parziale di <span class="math notranslate nohighlight">\( \ell \)</span> rispetto a <span class="math notranslate nohighlight">\( \mu \)</span>:</p>
<div class="math notranslate nohighlight">
\[
   \frac{\partial \ell}{\partial \mu} = \sum_{i=1}^n \frac{(y_i - \mu)}{\sigma^2}.
   \]</div>
</li>
<li><p>Impostiamo la derivata uguale a zero per trovare il punto di massimo:</p>
<div class="math notranslate nohighlight">
\[
   \frac{1}{\sigma^2} \sum_{i=1}^n (y_i - \mu) = 0.
   \]</div>
</li>
</ol>
<p>Risolvendo questa equazione per <span class="math notranslate nohighlight">\( \mu \)</span>, otteniamo la stima di massima verosimiglianza:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu}_{MLE} = \frac{1}{n} \sum_{i=1}^n y_i = \bar{y}.
\]</div>
<p>Questa formula ci mostra che la stima di massima verosimiglianza per <span class="math notranslate nohighlight">\( \mu \)</span> corrisponde semplicemente alla media aritmetica delle osservazioni.</p>
<p>Questo processo può essere analogamente applicato per stimare <span class="math notranslate nohighlight">\( \sigma^2 \)</span>, la varianza, e si trova che la stima di massima verosimiglianza per <span class="math notranslate nohighlight">\( \sigma^2 \)</span> è pari alla varianza campionaria.</p>
<p>In conclusione, all’interno di una distribuzione gaussiana, le stime di massima verosimiglianza per <span class="math notranslate nohighlight">\( \mu \)</span> (la media) e <span class="math notranslate nohighlight">\( \sigma^2 \)</span> (la varianza) coincidono con la media campionaria e la varianza campionaria, rispettivamente.</p>
<div class="exercise admonition" id="ex-likelihood-associative-learning">

<p class="admonition-title"><span class="caption-number">Exercise 142 </span></p>
<section id="exercise-content">
<p>Consideriamo un esempio relativo all’apprendimento per rinforzo. Lo scopo degli studi sull’apprendimento per rinforzo è quello di comprendere come le persone imparano a massimizzare le loro ricompense in situazioni in cui la scelta migliore è inizialmente sconosciuta. In modo più specifico, consideriamo il seguente problema di apprendimento. Un partecipante deve effettuare ripetutamente delle scelte tra diverse opzioni o azioni, e dopo ogni scelta riceve una ricompensa numerica estratta da una distribuzione di probabilità che dipende dall’azione selezionata. L’obiettivo del partecipante è massimizzare la ricompensa totale attesa durante un certo periodo di tempo, ad esempio, durante 100 scelte. Per descrivere questa situazione, viene spesso utilizzata la metafora di un giocatore che deve fare una serie di <span class="math notranslate nohighlight">\(T\)</span> scelte tra <span class="math notranslate nohighlight">\(K\)</span> slot machine (conosciute anche come «multi-armed bandits») al fine di massimizzare le sue vincite. Se nella scelta <span class="math notranslate nohighlight">\(t\)</span> viene selezionata la slot machine <span class="math notranslate nohighlight">\(k\)</span>, viene ottenuta una ricompensa <span class="math notranslate nohighlight">\(r_t\)</span> che ha valore <code class="docutils literal notranslate"><span class="pre">1</span></code> con una probabilità di successo <span class="math notranslate nohighlight">\(\mu^k_t\)</span>, altrimenti ha valore <code class="docutils literal notranslate"><span class="pre">0</span></code>. Le probabilità di successo sono diverse per ogni slot machine e inizialmente sono sconosciute al partecipante. Nella versione più semplice di questo compito, le probabilità di successo rimangono costanti nel tempo.</p>
<p>Il modello di Rescorla-Wagner è un modello di apprendimento associativo che descrive come gli animali o gli umani aggiornano le loro aspettative di rinforzo in risposta a stimoli. Il modello può essere descritto con la seguente formula di aggiornamento:</p>
<div class="math notranslate nohighlight">
\[ V_{t+1} = V_t + \alpha (\lambda - V_t), \]</div>
<p>dove:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V_t\)</span> è il valore predetto del rinforzo al tempo <span class="math notranslate nohighlight">\(t\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> è il tasso di apprendimento, un parametro che vogliamo stimare,</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> è l’intensità del rinforzo,</p></li>
<li><p><span class="math notranslate nohighlight">\(V_{t+1}\)</span> è il valore aggiornato dopo aver sperimentato il rinforzo.</p></li>
</ul>
<p>Per semplificare, consideriamo un caso in cui gli stimoli si presentano in maniera binaria (rinforzo presente o assente), e <span class="math notranslate nohighlight">\(\lambda\)</span> è noto. L’obiettivo è stimare il valore di <span class="math notranslate nohighlight">\(\alpha\)</span> che massimizza la verosimiglianza dei dati osservati sotto il modello.</p>
<p>La funzione di verosimiglianza per questo modello dipende dalla differenza tra i valori predetti e gli effettivi rinforzi ricevuti. Tuttavia, la formulazione esatta della funzione di verosimiglianza può variare a seconda della specifica formulazione del problema e dei dati disponibili. Per mantenere le cose semplici, consideriamo una versione semplificata in cui la «verosimiglianza» è basata sulla somma dei quadrati degli errori (SSE) tra i rinforzi previsti e quelli osservati (anche se tecnicamente questo non è un approccio basato sulla verosimiglianza nel senso statistico classico).</p>
<p>Per questo esempio, assumiamo di avere un semplice set di dati di rinforzi osservati e vogliamo trovare il valore di <span class="math notranslate nohighlight">\(\alpha\)</span> che minimizza l’SSE:</p>
<div class="math notranslate nohighlight">
\[ SSE = \sum_{t=1}^{n} (\lambda - V_t)^2. \]</div>
<p>Ecco un esempio di implementazione in Python che utilizza <code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize</span></code> per stimare <span class="math notranslate nohighlight">\(\alpha\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dati di esempio: rinforzi osservati (lambda)</span>
<span class="c1"># In questo esempio, assumiamo lambda = 1 per rinforzo presente e lambda = 0 per rinforzo assente</span>
<span class="c1"># per semplicità. In pratica, lambda potrebbe essere diverso a seconda degli esperimenti.</span>
<span class="n">rinforzi_osservati</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># Esempio di sequenza di rinforzi</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Funzione che calcola l&#39;SSE per un dato valore di alpha</span>
<span class="k">def</span> <span class="nf">sse</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">rinforzi</span><span class="p">,</span> <span class="n">V0</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">V0</span>
    <span class="n">sse</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">lambda_</span> <span class="ow">in</span> <span class="n">rinforzi</span><span class="p">:</span>
        <span class="n">sse</span> <span class="o">+=</span> <span class="p">(</span><span class="n">lambda_</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">V</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">lambda_</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># Aggiornamento del valore secondo il modello Rescorla-Wagner</span>
    <span class="k">return</span> <span class="n">sse</span>

<span class="c1"># Ottimizzazione per trovare il valore di alpha che minimizza l&#39;SSE</span>
<span class="n">result_alpha</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">sse</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">rinforzi_osservati</span><span class="p">,))</span>

<span class="c1"># Il risultato ottimizzato per alpha</span>
<span class="n">result_alpha</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.29739989])
</pre></div>
</div>
</div>
</div>
<p>Il valore di <span class="math notranslate nohighlight">\(\alpha\)</span> (tasso di apprendimento) che minimizza la somma dei quadrati degli errori (SSE) per il modello di Rescorla-Wagner, dato il campione di rinforzi osservati, è circa <span class="math notranslate nohighlight">\(0.297\)</span>. Questo suggerisce che il tasso di apprendimento ottimale per adattare il modello ai dati osservati in questo esempio semplificato è di circa 0.297, secondo l’approccio di minimizzazione dell’errore utilizzato qui.</p>
</section>
</div>
<div class="exercise admonition" id="ex-likelihood-exponential distribution">

<p class="admonition-title"><span class="caption-number">Exercise 143 </span></p>
<section id="exercise-content">
<p>Consideriamo ora un esempio relativo alla distribuzione esponenziale. Supponiamo che i seguenti siano i tempi di attesa per un certo evento:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">27</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Definiamo la funzione di log-verosimiglianza negativa. Per iniziare, ricordiamo che la funzione di densità di probabilità (PDF) per una distribuzione esponenziale, dato un tasso <span class="math notranslate nohighlight">\(\lambda\)</span>, è definita come:</p>
<div class="math notranslate nohighlight">
\[
f(x; \lambda) = \lambda e^{-\lambda x} \quad \text{per } x \geq 0.
\]</div>
<p>La verosimiglianza (<span class="math notranslate nohighlight">\(L\)</span>) di osservare un insieme di dati <span class="math notranslate nohighlight">\(\{x_1, x_2, ..., x_n\}\)</span> dato un parametro <span class="math notranslate nohighlight">\(\lambda\)</span> è il prodotto delle funzioni di densità di probabilità per ogni punto dati, assumendo che ciascun dato sia indipendente dagli altri. Quindi, per <span class="math notranslate nohighlight">\(n\)</span> dati osservati, la funzione di verosimiglianza è:</p>
<div class="math notranslate nohighlight">
\[
L(\lambda) = \prod_{i=1}^{n} f(x_i; \lambda) = \prod_{i=1}^{n} \lambda e^{-\lambda x_i}
\]</div>
<p>La log-verosimiglianza (<span class="math notranslate nohighlight">\(\log(L(\lambda))\)</span>) è il logaritmo naturale di <span class="math notranslate nohighlight">\(L(\lambda)\)</span>. Utilizziamo il logaritmo per semplificare la moltiplicazione in una somma, il che rende più semplici sia il calcolo che la differenziazione. Pertanto, la log-verosimiglianza diventa:</p>
<div class="math notranslate nohighlight">
\[
\log(L(\lambda)) = \log\left(\prod_{i=1}^{n} \lambda e^{-\lambda x_i}\right) = \sum_{i=1}^{n} \log(\lambda e^{-\lambda x_i}) = \sum_{i=1}^{n} (\log(\lambda) - \lambda x_i)
\]</div>
<p>Il motivo per utilizzare il negativo della log-verosimiglianza, cioè <span class="math notranslate nohighlight">\(-\log(L(\lambda))\)</span>, nelle tecniche di ottimizzazione, è perché molte librerie e funzioni di ottimizzazione sono progettate per minimizzare una funzione obiettivo piuttosto che massimizzarla. Dato che vogliamo trovare il valore di <span class="math notranslate nohighlight">\(\lambda\)</span> che massimizza la log-verosimiglianza (e quindi la verosimiglianza), possiamo invece minimizzare il suo negativo. Di conseguenza, la funzione obiettivo che passiamo all’algoritmo di minimizzazione è:</p>
<div class="math notranslate nohighlight">
\[
-\log(L(\lambda)) = -\sum_{i=1}^{n} (\log(\lambda) - \lambda x_i)
\]</div>
<p>Scriviamo la funzione in Python:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">neg_log_likelihood</span><span class="p">(</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
    <span class="n">lambda_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># Assicura che lambda_ sia almeno eps</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lambda_</span><span class="p">)</span> <span class="o">-</span> <span class="n">lambda_</span> <span class="o">*</span> <span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Minimizzaziamo la funzione di log-verosimiglianza negativa:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">neg_log_likelihood</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="p">,),</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Il valore di lambda che massimizza la log-verosimiglianza è: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Il valore di lambda che massimizza la log-verosimiglianza è: 0.04166666292998713
</pre></div>
</div>
</div>
</div>
<p>Avendo trovato il tasso <span class="math notranslate nohighlight">\(\lambda\)</span>, la stima di massima verosimiglianza del tempo di attesa medio diventa:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([24.00000215])
</pre></div>
</div>
</div>
</div>
<p>Visualizzazione.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lambda_opt</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">lambda_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">LL</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="n">neg_log_likelihood</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">L</span> <span class="ow">in</span> <span class="n">lambda_array</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_array</span><span class="p">,</span> <span class="n">LL</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Log-likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">lambda_opt</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Optimal $\lambda$ = </span><span class="si">{</span><span class="n">lambda_opt</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$\lambda$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Log-likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Log-likelihood over a range of $\lambda$ values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&gt;:6: SyntaxWarning: invalid escape sequence &#39;\l&#39;
&lt;&gt;:7: SyntaxWarning: invalid escape sequence &#39;\l&#39;
&lt;&gt;:9: SyntaxWarning: invalid escape sequence &#39;\l&#39;
&lt;&gt;:6: SyntaxWarning: invalid escape sequence &#39;\l&#39;
&lt;&gt;:7: SyntaxWarning: invalid escape sequence &#39;\l&#39;
&lt;&gt;:9: SyntaxWarning: invalid escape sequence &#39;\l&#39;
/var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/ipykernel_32971/73438383.py:6: SyntaxWarning: invalid escape sequence &#39;\l&#39;
  plt.axvline(lambda_opt, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=f&#39;Optimal $\lambda$ = {lambda_opt:.4f}&#39;)
/var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/ipykernel_32971/73438383.py:7: SyntaxWarning: invalid escape sequence &#39;\l&#39;
  plt.xlabel(&#39;$\lambda$&#39;)
/var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/ipykernel_32971/73438383.py:9: SyntaxWarning: invalid escape sequence &#39;\l&#39;
  plt.title(&#39;Log-likelihood over a range of $\lambda$ values&#39;)
</pre></div>
</div>
<a class="reference internal image-reference" href="../_images/df09e3bfc6ea17582c2fea53fdae8a0581f339f9ba3592a679fe0733ce1c58de.png"><img alt="../_images/df09e3bfc6ea17582c2fea53fdae8a0581f339f9ba3592a679fe0733ce1c58de.png" src="../_images/df09e3bfc6ea17582c2fea53fdae8a0581f339f9ba3592a679fe0733ce1c58de.png" style="width: 731px; height: 491px;" /></a>
</div>
</div>
</section>
</div>
</section>
</section>
<section id="conclusione-e-riflessioni-finali">
<h2>Conclusione e Riflessioni Finali<a class="headerlink" href="#conclusione-e-riflessioni-finali" title="Link to this heading">#</a></h2>
<p>La funzione di verosimiglianza rappresenta un elemento cruciale che collega i dati osservati ai parametri di un modello statistico. Essa fornisce una misura della plausibilità dei dati in relazione a diversi valori possibili dei parametri del modello. La strutturazione di una funzione di verosimiglianza richiede la considerazione di tre componenti fondamentali: il modello statistico che si presume abbia generato i dati, l’insieme di valori possibili per i parametri di tale modello e le osservazioni empiriche che effettivamente abbiamo a disposizione.</p>
<p>La funzione di verosimiglianza è centrale nella pratica dell’inferenza statistica. Essa ci permette di quantificare quanto bene differenti set di parametri potrebbero aver generato i dati osservati. Questo è fondamentale sia per la selezione del modello che per la stima dei parametri, e pertanto è indispensabile per un’analisi dati rigorosa e per un’interpretazione accurata dei risultati.</p>
<p>Un’applicazione pratica e illustrativa dei principi esposti in questo capitolo è fornita nella sezione sul modello Rescorla-Wagner, che è un esempio di come la teoria della verosimiglianza possa essere applicata per affrontare questioni empiriche in psicologia.</p>
<p>In sintesi, la comprensione e l’applicazione appropriata della funzione di verosimiglianza sono passaggi essenziali nel processo di analisi dati. Essa costituisce uno strumento indispensabile per chi è impegnato nella ricerca empirica e nell’interpretazione di dati complessi.</p>
<div class="admonition-esercizi admonition">
<p class="admonition-title">Esercizi</p>
<p>All’esame ti verrà chiesto di:</p>
<ul class="simple">
<li><p>Calcolare la funzione di verosimiglianza binomiale e riportare il valore della funzione in corrispondenza di specifici valori <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p>Calcolare la funzione di verosimiglianza del modello gaussiano, per <span class="math notranslate nohighlight">\(\sigma\)</span> noto, e riportare il valore della funzione in corrispondenza di specifici valori <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>Calcolare la stima di massima verosimiglianza.</p></li>
<li><p>Rispondere a domande che implicano una adeguata comprensione del concetto di funzione di verosimiglianza.</p></li>
</ul>
</div>
</section>
<section id="informazioni-sull-ambiente-di-sviluppo">
<h2>Informazioni sull’Ambiente di Sviluppo<a class="headerlink" href="#informazioni-sull-ambiente-di-sviluppo" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> -n -u -v -iv -w -m
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Last updated: Sun Jun 16 2024

Python implementation: CPython
Python version       : 3.12.3
IPython version      : 8.25.0

Compiler    : Clang 16.0.6 
OS          : Darwin
Release     : 23.4.0
Machine     : arm64
Processor   : arm
CPU cores   : 8
Architecture: 64bit

numpy     : 1.26.4
matplotlib: 3.8.4
seaborn   : 0.13.2
pandas    : 2.2.2
arviz     : 0.18.0
scipy     : 1.13.1

Watermark: 2.4.3
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_3"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="E_beta_distr.html"
       title="pagina precedente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">precedente</p>
        <p class="prev-next-title">✏️ Esercizi</p>
      </div>
    </a>
    <a class="right-next"
       href="E_likelihood.html"
       title="pagina seguente">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">successivo</p>
        <p class="prev-next-title">✏️ Esercizi</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contenuti
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparazione-del-notebook">Preparazione del Notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#il-principio-della-verosimiglianza-e-la-sua-formalizzazione">Il Principio della Verosimiglianza e la sua Formalizzazione</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verosimiglianza-binomiale">Verosimiglianza Binomiale</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretazione-della-funzione-di-verosimiglianza">Interpretazione della Funzione di Verosimiglianza</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#massima-verosimiglianza">Massima verosimiglianza</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-funzione-di-log-verosimiglianza">La Funzione di Log-Verosimiglianza</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#verosimiglianza-congiunta">Verosimiglianza Congiunta</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-verosimiglianza-marginale">La Verosimiglianza Marginale</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modello-gaussiano-e-verosimiglianza">Modello Gaussiano e Verosimiglianza</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#caso-di-una-singola-osservazione">Caso di una Singola Osservazione</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana">Campione indipendente di osservazioni da una distribuzione gaussiana</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-stima-di-massima-verosimiglianza-per-mu">La Stima di Massima Verosimiglianza per <span class="math notranslate nohighlight">\( \mu \)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusione-e-riflessioni-finali">Conclusione e Riflessioni Finali</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#informazioni-sull-ambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Di Corrado Caudek
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>