
<!DOCTYPE html>


<html lang="it" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Entropia &#8212; ds4p</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css?v=20b57f81" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../_static/documentation_options.js?v=8d586cc4"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/translations.js?v=0173e136"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-TP2WLBPMS6"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-TP2WLBPMS6');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-TP2WLBPMS6');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_5/05_30_entropy';</script>
    <link rel="canonical" href="https://ccaudek.github.io/ds4p/chapter_5/05_30_entropy.html" />
    <link rel="icon" href="../_static/increasing.png"/>
    <link rel="index" title="Indice" href="../genindex.html" />
    <link rel="search" title="Cerca" href="../search.html" />
    <link rel="next" title="La divergenza di Kullback-Leibler" href="05_31_kl.html" />
    <link rel="prev" title="Modello di mediazione con Stan" href="05_26_stan_mediation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="it"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="ds4p - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="ds4p - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Cerca" aria-label="Cerca" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Cerca</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Benvenuti
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_1/introduction_chapter_1.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/00_prelims.html">Preliminari</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/01_python_1.html">Python (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/02_python_2.html">Python (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_python.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/03_numpy.html">NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_numpy.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/04_pandas.html">Pandas (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/05_pandas_aggregate.html">Pandas (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/06_pandas_functions.html">Pandas (3)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_pandas.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/07_matplotlib.html">Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/08_seaborn.html">Seaborn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_matplotlib.html">✏️ Esercizi</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_2/introduction_chapter_2.html">Statistica descrittiva</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/00_scientific_method.html">La scienza dei dati e il metodo scientifico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/01_key_notions.html">Concetti chiave</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_key_notions.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/02_measurement.html">La misurazione in psicologia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_scales.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/03_freq_distr.html">Dati e frequenze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_sums.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/04_loc_scale.html">Indici di posizione e di scala</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/05_correlation.html">Le relazioni tra variabili</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/06_causality.html">Lo studio delle cause dei fenomeni</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_eda.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_mehr_song_spelke.html">✏️ Esercizi</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_3/introduction_chapter_3.html">Probabilità</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/01_intro_prob.html">Introduzione al calcolo delle probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/02_conditional_prob.html">Probabilità condizionata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_cond_prob_1.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_cond_prob_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_discrete_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/03_bayes_theorem.html">Il teorema di Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_bayes_theorem.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_bayes_theorem_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/04a_random_var.html">Introduzione alle variabili casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/04b_expval_var.html">Proprietà delle variabili casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_exp_val_variance.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/04c_sampling_distr.html">Stime, stimatori e parametri</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_rv_discrete.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/05_joint_prob.html">Probabilità congiunta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_joint_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_covariance.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/06_density_func.html">La funzione di densità di probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/07_discr_rv_distr.html">Distribuzioni di v.c. discrete</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_binomial.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/08_cont_rv_distr.html">Distribuzioni di v.c. continue</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_gaussian.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_beta_distr.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/09_likelihood.html">La verosimiglianza</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_likelihood.html">✏️ Esercizi</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_4/introduction_part_4.html">Inferenza bayesiana</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/01_intro_bayes.html">Modellazione bayesiana</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/02_subj_prop.html">Pensare ad una proporzione in termini soggettivi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/02_grid_gauss.html">Verosimiglianza Gaussiana: Metodo Basato su Griglia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/03_conjugate_families_1.html">Distribuzioni coniugate</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_conjugate_families_1.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/04_conjugate_families_2.html">Distribuzioni coniugate (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/05_summary_posterior.html">Sintesi a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_conjugate.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/06_balance-prior-post.html">L’influenza della distribuzione a priori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/10_metropolis.html">Monte Carlo a Catena di Markov</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/15_stan_beta_binomial.html">Introduzione a Stan</a></li>

<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_stan_beta_binomial.html">✏️ Esercizio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/16_stan_summary_posterior.html">Metodi di sintesi della distribuzione a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/17_stan_diagnostics.html">Diagnostica delle catene markoviane</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/18_stan_prediction.html">La predizione bayesiana</a></li>

<li class="toctree-l2"><a class="reference internal" href="../chapter_4/19_stan_odds_ratio.html">Analisi bayesiana dell’odds-ratio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/22_stan_normal_normal.html">Inferenza bayesiana su una media</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/23_stan_two_groups.html">Confronto tra due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/24_stan_hier_beta_binom.html">Modello gerarchico beta-binomiale con Stan</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="introduction_part_5.html">Analisi della regressione</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="05_03_reglin_bayesian.html">Analisi bayesiana del modello di regressione lineare bivariato</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_reglin_1.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_04_synt_sugar.html">Zucchero sintattico</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_05_two_means.html">Confronto tra le medie di due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_reglin_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_reglin_3.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_reglin_4.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_06_hier_regr.html">Il modello lineare gerarchico</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_07_robust_regr.html">Regressione robusta</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_08_specification_error.html">Errore di specificazione</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_09_causal_inference.html">Inferenza causale</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_causal_inference.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_22_stan_logistic_regr.html">Regressione logistica con Stan</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_24_stan_mixed_models.html">Modelli misti con Stan</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_25_stan_rct.html">Incorporare dati storici di controllo in una RCT</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_26_stan_mediation.html">Modello di mediazione con Stan</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Entropia</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_31_kl.html">La divergenza di Kullback-Leibler</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_kl.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_32_stan_loo.html">Validazione Incrociata Leave-One-Out</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_35_missing.html">Dati mancanti</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_40_rescorla_wagner.html">Apprendimento per rinforzo</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_6/introduction_part_6.html">Inferenza frequentista</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/01_intro_frequentist.html">Introduzione all’inferenza frequentista</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_estimation.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/02_conf_interv.html">Intervallo di confidenza</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_conf_interv.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/03_test_ipotesi.html">Significatività statistica</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_t_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_interpretation_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_significato_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/04_two_ind_samples.html">Test t di Student per campioni indipendenti</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_test_media_pop.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_ampie.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_piccoli.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_campioni_appaiati.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_confronto_proporzioni.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/05_crisis.html">La crisi della generalizzabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/06_limiti_stat_frequentista.html">Limiti dell’inferenza frequentista</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/07_effect_size.html">La grandezza dell’effetto</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/09_s_m_errors.html">Crisi della replicabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/10_integrity.html">Integrità della ricerca</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../references/bibliography.html">Bibliografia</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_7/introduction_appendix.html">Appendici</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a00_installation.html">Ambiente di lavoro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a01_markdown.html">Jupyter Notebook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a02_shell.html">La Shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a03_colab_tutorial.html">Colab: un breve tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a04_virtual_env.html">Ambienti virtuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a10_math_symbols.html">Simbologia di base</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a11_numbers.html">Numeri binari, interi, razionali, irrazionali e reali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a12_sum_notation.html">Simbolo di somma (sommatorie)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a13_sets.html">Insiemi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a13a_probability.html">Sigma algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a14_combinatorics.html">Calcolo combinatorio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a15_calculus.html">Per liberarvi dai terrori preliminari</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a20_kde_plot.html">Kernel Density Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a30_prob_tutorial.html">Esercizi di probabilità discreta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a40_rng.html">Generazione di numeri casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a44_montecarlo.html">Simulazione Monte Carlo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a45_mcmc.html">Catene di Markov</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a46_stan.html">Linguaggio Stan</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a50_lin_fun.html">La funzione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a50_reglin_ml.html">Modello di Regressione Bivariato e ML</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a51_reglin_1.html">Regressione lineare bivariata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a60_ttest_exercises.html">Esercizi sull’inferenza frequentista</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a70_predict_counts.html">La predizione delle frequenze</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/ccaudek/ds4p/blob/main/docs/chapter_5/05_30_entropy.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Scarica questa pagina">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_5/05_30_entropy.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Scarica il file sorgente"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Stampa in PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Modalità schermo intero"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Cerca" aria-label="Cerca" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Entropia</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contenuti </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparazione-del-notebook">Preparazione del Notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-generalizzabilita-dei-modelli-e-il-metodo-scientifico">La Generalizzabilità dei Modelli e il Metodo Scientifico</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cos-e-l-entropia-dell-informazione">Cos’è l’Entropia dell’Informazione?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additivita-dell-entropia-per-eventi-indipendenti">Additività dell’Entropia per Eventi Indipendenti</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropia-di-variabili-casuali">Entropia di Variabili Casuali</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#entropia-di-una-variabile-casuale-discreta">Entropia di una Variabile Casuale Discreta</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#entropia-di-una-variabile-casuale-continua">Entropia di una Variabile Casuale Continua</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applicazioni-psicologiche">Applicazioni Psicologiche</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#divergenza-di-kullback-leibler-uno-strumento-per-confrontare-distribuzioni-probabilistiche">Divergenza di Kullback-Leibler: Uno Strumento per Confrontare Distribuzioni Probabilistiche</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calcolo-della-divergenza-kl">Calcolo della Divergenza KL</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applicazione-della-divergenza-kl-nella-selezione-di-modelli">Applicazione della Divergenza KL nella Selezione di Modelli</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#proprieta-importanti">Proprietà Importanti</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#selezione-dei-modelli-statistici">Selezione dei Modelli Statistici</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#riflessioni-conclusive">Riflessioni Conclusive</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#informazioni-sull-ambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="entropia">
<span id="entropy-notebook"></span><h1>Entropia<a class="headerlink" href="#entropia" title="Link to this heading">#</a></h1>
<div class="admonition-obiettivi-di-apprendimento admonition">
<p class="admonition-title">Obiettivi di apprendimento</p>
<p>Dopo aver completato questo capitolo, acquisirete le competenze per:</p>
<ul class="simple">
<li><p>comprendere il concetto di entropia;</p></li>
<li><p>comprendere il concetto di divervenza di Kullback-Leibler (KL);</p></li>
<li><p>calcolare la divergenza KL dall’entropia;</p></li>
</ul>
</div>
<p>Nel contesto della statistica bayesiana, è cruciale confrontare diversi modelli predittivi per identificare quello che meglio si adatta ai dati disponibili. Una metrica essenziale in questo confronto è la Expected Log Predictive Density (ELPD), che misura l’accuratezza con cui un modello può prevedere nuovi dati. Non essendo possibile calcolare direttamente l’ELPD, a causa della necessità di conoscere il meccanismo generatore dei dati <span class="math notranslate nohighlight">\( p_t(y) \)</span>, ci affidiamo a una stima approssimativa fornita dalla distribuzione predittiva a posteriori del modello, <span class="math notranslate nohighlight">\( p(\tilde{y} | y) \)</span>.</p>
<p>Per ottenere una stima più accurata della capacità di generalizzazione di un modello su futuri set di dati, utilizziamo metodi di stima dell’ELPD basati sulla validazione incrociata. Questa tecnica consiste nell’addestrare il modello su un sottoinsieme di dati e testarlo su un altro, isolando così le prestazioni del modello dalle variazioni casuali presenti nei dati. Il risultato di questo processo è l’indice di Leave-One-Out Cross-Validation (LOO-CV), fondamentale per comparare diversi modelli.</p>
<p>La differenza nei valori di Leave-One-Out Cross-Validation (LOO-CV) tra due modelli, accompagnata dal calcolo dell’errore standard associato a questa differenza, ci consente di determinare se esiste una differenza robusta nelle prestazioni tra i due modelli. Se il rapporto tra questa differenza di LOO-CV e il relativo errore standard supera il valore di 2, possiamo concludere che i modelli mostrano differenze sostanziali. Questo indica che le variazioni osservate non sono casuali ma riflettono una superiorità effettiva di un modello rispetto all’altro.</p>
<p>In questo capitolo, esploreremo il concetto di entropia, essenziale per quantificare l’incertezza nelle distribuzioni di probabilità. L’entropia di una variabile casuale rappresenta la media della sua imprevedibilità. Approfondiremo anche il modo in cui l’entropia può essere impiegata per misurare la «distanza» tra un modello teorico e i dati osservati, introducendo il concetto di divergenza di Kullback-Leibler (KL). Questa metrica quantifica le discrepanze tra due distribuzioni probabilistiche, fornendo una misura di quanto efficacemente un modello rappresenti le osservazioni empiriche. Il capitolo successivo presenterà un’analisi della tecnica di Validazione Incrociata Leave-One-Out, impiegata per calcolare un’approssimazione della divergenza KL, nota come LOO-CV.</p>
<section id="preparazione-del-notebook">
<h2>Preparazione del Notebook<a class="headerlink" href="#preparazione-del-notebook" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">kl_div</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">8927</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;arviz-darkgrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="la-generalizzabilita-dei-modelli-e-il-metodo-scientifico">
<h2>La Generalizzabilità dei Modelli e il Metodo Scientifico<a class="headerlink" href="#la-generalizzabilita-dei-modelli-e-il-metodo-scientifico" title="Link to this heading">#</a></h2>
<p>La generalizzabilità dei modelli è un concetto chiave nella scienza, essendo uno dei fondamenti del metodo scientifico. Questo principio si riferisce alla capacità di un modello di applicarsi e produrre risultati validi oltre il contesto specifico o il set di dati in cui è stato originariamente sviluppato o testato. Il valore scientifico di un modello è quindi fortemente influenzato dalla sua capacità di generalizzarsi a nuovi dati.</p>
<p>Nella pratica, la generalizzabilità di un modello può essere minacciata da due problemi principali: il sotto-adattamento e il sovra-adattamento. Il sotto-adattamento si verifica quando un modello è troppo semplice per catturare adeguatamente la complessità dei dati, portando a prestazioni insoddisfacenti sia sui dati di addestramento che su nuovi insiemi di dati. Questo limita gravemente la sua utilità in applicazioni pratiche. Al contrario, il sovra-adattamento si manifesta quando un modello è eccessivamente complesso, adattandosi troppo fedelmente al rumore o alle peculiarità specifiche del set di dati di addestramento a discapito della capacità di generalizzare a nuovi dati.</p>
<p>L’approccio bayesiano alla modellazione consente di gestire in modo efficace la necessità di un compromesso tra complessità del modello e adattamento ai dati. La selezione di modelli, come descritto da <span id="id1">McElreath [<a class="reference internal" href="../references/bibliography.html#id14" title="Richard McElreath. Statistical rethinking: A Bayesian course with examples in R and Stan. CRC Press, Boca Raton, Florida, 2nd edition edition, 2020.">McE20</a>]</span>, è un processo che richiede di mediare tra la semplicità del modello e la sua capacità di rappresentare fedelmente la realtà dei dati.</p>
<p>Una pratica comune nella scelta tra modelli alternativi si basa sul principio del rasoio di Ockham, che predilige le spiegazioni più semplici in presenza di multiple teorie equivalenti per un fenomeno. Tuttavia, questo principio da solo non è sufficiente: è essenziale che il modello scelto descriva accuratamente i dati.</p>
<p>La metodologia prevalente nella selezione dei modelli è spesso centrata sull’uso dei valori-p, ma come evidenziato da <span id="id2">[<a class="reference internal" href="../references/bibliography.html#id14" title="Richard McElreath. Statistical rethinking: A Bayesian course with examples in R and Stan. CRC Press, Boca Raton, Florida, 2nd edition edition, 2020.">McE20</a>]</span>, questo approccio è problematico e privo di una solida giustificazione teorica.</p>
<p>Un metodo più robusto e fondato scientificamente impiega invece la divergenza di Kullback-Leibler, una misura che valuta quanto un modello approssimi efficacemente la distribuzione reale dei dati, offrendo una stima quantitativa della sua aderenza al processo generativo sottostante. Questo capitolo pone le basi per comprendere il concetto di entropia, essenziale per affrontare nel prossimo capitolo la divergenza di Kullback-Leibler e le sue implicazioni nella selezione di modelli.</p>
</section>
<section id="cos-e-l-entropia-dell-informazione">
<h2>Cos’è l’Entropia dell’Informazione?<a class="headerlink" href="#cos-e-l-entropia-dell-informazione" title="Link to this heading">#</a></h2>
<p>L’entropia dell’informazione, un concetto introdotto da Claude Shannon, rappresenta uno dei fondamenti della teoria dell’informazione. Questa grandezza matematica quantifica l’incertezza o la sorpresa associata alla ricezione di un messaggio, misurando quanto sia sorprendente un evento in base alla sua probabilità. Gli eventi che si verificano con alta probabilità sono considerati meno sorprendenti perché prevedibili; al contrario, quelli meno probabili, essendo inaspettati, trasmettono più sorpresa.</p>
<p>La sorpresa di un evento, determinata dalla sua probabilità <span class="math notranslate nohighlight">\( p \)</span>, si calcola con la formula:</p>
<div class="math notranslate nohighlight">
\[ H(p) = -\log_2(p) = \log_2 \left(\frac{1}{p}\right). \]</div>
<p>L’uso del logaritmo in questa formula ha diverse giustificazioni:</p>
<ul class="simple">
<li><p>Il logaritmo converte la moltiplicazione delle probabilità in una somma. Questo semplifica l’analisi di eventi complessi formati da più eventi indipendenti.</p></li>
<li><p>La base del logaritmo (in questo caso, 2) corrisponde all’unità di misura dell’informazione. La base 2 è utilizzata perché l’informazione viene misurata in bit, che rappresentano decisioni binarie.</p></li>
<li><p>La scala logaritmica riflette meglio la percezione umana dell’informazione e della sorpresa. Eventi con probabilità molto basse hanno un impatto informativo molto maggiore rispetto a variazioni di probabilità in range più alti.</p></li>
</ul>
<p>È importante notare che la base del logaritmo può variare: non ci sono unità intrinseche per misurare la sorpresa. Ad esempio, l’uso della base 2, comune nelle telecomunicazioni, porta a misurare l’informazione in «bit». Al contrario, l’adozione della base <span class="math notranslate nohighlight">\( e \)</span>, tipica nella fisica statistica, porta a misurazioni in «nats», o «cifre naturali».</p>
<p>Per illustrare, consideriamo alcuni esempi pratici.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">calcola_entropia</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>  <span class="c1"># Non c&#39;è incertezza se l&#39;evento è certo o impossibile</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">p</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

<span class="c1"># Esempi di probabilità</span>
<span class="n">probabilità</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>

<span class="c1"># Calcolo dell&#39;entropia per ciascuna probabilità</span>
<span class="n">entropie</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span> <span class="n">calcola_entropia</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">probabilità</span><span class="p">}</span>

<span class="nb">print</span><span class="p">(</span><span class="n">entropie</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{0.0: 0, 0.1: 0.33219280948873625, 0.5: 0.5, 0.9: 0.13680278410054494, 1.0: 0}
</pre></div>
</div>
</div>
</div>
<p>L’output di questo script mostra che l’entropia è massima per eventi con probabilità intermedia (0.5) e minima (zero) per eventi certi o impossibili.</p>
<p>In generale, possiamo dunque dire che l’entropia raggiunge il suo valore massimo in condizioni di completa equiprobabilità, ovvero quando ogni esito possibile di un evento ha esattamente la stessa probabilità di verificarsi. Questa condizione rappresenta il massimo grado di imprevedibilità, poiché non esistono indizi che possano aiutare a prevedere quale esito si verificherà.</p>
<p>Al contrario, l’entropia è minima, assumendo un valore di zero, quando l’esito di un evento è completamente certo. Questo avviene quando uno degli esiti possibili ha una probabilità di 1, eliminando qualsiasi forma di incertezza o sorpresa. In pratica, ciò significa che non c’è alcuna informazione da guadagnare nell’osservare l’evento, poiché l’esito è già noto in anticipo.</p>
<section id="additivita-dell-entropia-per-eventi-indipendenti">
<h3>Additività dell’Entropia per Eventi Indipendenti<a class="headerlink" href="#additivita-dell-entropia-per-eventi-indipendenti" title="Link to this heading">#</a></h3>
<p>L’entropia mostra una proprietà di additività nel caso di eventi indipendenti. Questo significa che, se due o più eventi indipendenti si verificano, l’entropia totale associata alla loro combinazione è uguale alla somma delle entropie di ciascun evento preso singolarmente. Questa caratteristica deriva dalla proprietà additiva dei logaritmi, che permette di sommare le entropie individuali per ottenere l’entropia complessiva.</p>
</section>
<section id="entropia-di-variabili-casuali">
<h3>Entropia di Variabili Casuali<a class="headerlink" href="#entropia-di-variabili-casuali" title="Link to this heading">#</a></h3>
<p>L’informazione di Shannon misura la sorpresa di un singolo evento, ma è possibile estendere questo concetto al caso di una distribuzione di probabilità, ovvero al caso di una variabile casuale discreta o continua. L’entropia fornisce una misura complessiva dell’incertezza o della sorpresa associata a una variabile casuale.</p>
<section id="entropia-di-una-variabile-casuale-discreta">
<h4>Entropia di una Variabile Casuale Discreta<a class="headerlink" href="#entropia-di-una-variabile-casuale-discreta" title="Link to this heading">#</a></h4>
<p>Consideriamo una variabile casuale discreta <span class="math notranslate nohighlight">\( X \)</span> che può assumere i valori <span class="math notranslate nohighlight">\( a_1, a_2, \ldots, a_n \)</span> con le relative probabilità <span class="math notranslate nohighlight">\( p_1, p_2, \ldots, p_n \)</span>, dove la somma totale delle probabilità è 1. L’entropia di <span class="math notranslate nohighlight">\( X \)</span> è calcolata come la somma pesata delle entropie di ciascun possibile esito:</p>
<div class="math notranslate nohighlight">
\[ H(X) = -\sum_{i=1}^{n} p_i \log_2(p_i). \]</div>
<p>La formula somma le informazioni di tutti i possibili esiti, pesando ciascun termine con la probabilità <span class="math notranslate nohighlight">\( p_i \)</span> dell’esito stesso. Questo significa che gli esiti più probabili influenzano maggiormente l’entropia totale rispetto a quelli meno probabili.</p>
<p>Il logaritmo converte la moltiplicazione delle probabilità in una somma, semplificando i calcoli per eventi indipendenti.</p>
<p>Il segno negativo è necessario perché i logaritmi delle probabilità, essendo numeri minori di 1, sono negativi. Il segno negativo inverte questi valori, trasformandoli in quantità positive che rappresentano correttamente l’informazione o la sorpresa. Inoltre, esiti più probabili, avendo <span class="math notranslate nohighlight">\( p_i \)</span> maggiori, producono logaritmi negativi meno estremi, riflettendo la loro minore sorpresa.</p>
<p>In sintesi, l’entropia <span class="math notranslate nohighlight">\( H(X) \)</span> misura l’incertezza complessiva di una variabile casuale discreta, tenendo conto delle probabilità di tutti i suoi possibili esiti. Ogni termine della somma <span class="math notranslate nohighlight">\( -p_i \log_2(p_i) \)</span> rappresenta la quantità di sorpresa o informazione associata a ciascun esito, ponderata dalla probabilità di quell’esito.</p>
</section>
<section id="entropia-di-una-variabile-casuale-continua">
<h4>Entropia di una Variabile Casuale Continua<a class="headerlink" href="#entropia-di-una-variabile-casuale-continua" title="Link to this heading">#</a></h4>
<p>Nel caso delle variabili casuali continue, il concetto di entropia viene generalizzato sostituendo la somma con un integrale. Questo è necessario perché le variabili continue possono assumere un numero infinito di valori all’interno di un intervallo.</p>
<p>Per una variabile casuale continua <span class="math notranslate nohighlight">\( X \)</span> con una funzione di densità di probabilità <span class="math notranslate nohighlight">\( p(x) \)</span>, l’entropia (nota anche come entropia differenziale) è definita dalla seguente formula:</p>
<div class="math notranslate nohighlight">
\[ H(X) = -\int p(x) \log_2(p(x)) \, dx, \]</div>
<p>dove:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( p(x) \)</span> è la funzione di densità di probabilità di <span class="math notranslate nohighlight">\( X \)</span>,</p></li>
<li><p>l’integrale è calcolato su tutto il dominio di <span class="math notranslate nohighlight">\( X \)</span>.</p></li>
</ul>
<p>L’entropia di una variabile casuale continua fornisce una misura dell’incertezza o della sorpresa associata alla distribuzione della variabile. Come nel caso discreto, l’entropia continua quantifica l’incertezza associata a <span class="math notranslate nohighlight">\( X \)</span>. Una PDF molto concentrata (ad esempio, una distribuzione con picchi stretti) implica bassa entropia, poiché l’evento è più prevedibile. Una PDF distribuita uniformemente implica alta entropia, poiché l’evento è meno prevedibile.</p>
<p>Il segno negativo assicura che l’entropia sia una quantità positiva, in quanto <span class="math notranslate nohighlight">\( \log_2(p(x)) \)</span> è negativo per <span class="math notranslate nohighlight">\( p(x) \)</span> compreso tra 0 e 1.</p>
</section>
</section>
<section id="applicazioni-psicologiche">
<h3>Applicazioni Psicologiche<a class="headerlink" href="#applicazioni-psicologiche" title="Link to this heading">#</a></h3>
<p>L’entropia dell’informazione trova applicazioni anche in psicologia, per esempio nello studio dell’effetto della sorpresa sull’umore. La sorpresa, o entropia, è stata documentata sia in laboratorio che in contesti naturali come un fattore significativo che influenza le emozioni.</p>
<p>Ad esempio, <span id="id3">Spector [<a class="reference internal" href="../references/bibliography.html#id3" title="Aaron J Spector. Expectations, fulfillment, and morale. The Journal of Abnormal and Social Psychology, 52(1):51–56, 1956.">Spe56</a>]</span> osservò l’effetto della probabilità a priori sulla soddisfazione dei soggetti in risposta a una promozione lavorativa. I risultati indicano che gli esiti meno probabili a priori (e quindi più sorprendenti quando si verificano) hanno un impatto maggiore sull’umore. In altre parole, quando un evento inatteso e sorprendente si verifica, esso tende a influenzare l’umore in modo più forte rispetto a eventi previsti e probabili.</p>
</section>
<section id="divergenza-di-kullback-leibler-uno-strumento-per-confrontare-distribuzioni-probabilistiche">
<h3>Divergenza di Kullback-Leibler: Uno Strumento per Confrontare Distribuzioni Probabilistiche<a class="headerlink" href="#divergenza-di-kullback-leibler-uno-strumento-per-confrontare-distribuzioni-probabilistiche" title="Link to this heading">#</a></h3>
<p>La Divergenza di Kullback-Leibler (KL), introdotta da Kullback e Leibler nel 1951, estende il concetto di entropia di Shannon. Mentre l’entropia misura l’incertezza di una singola distribuzione di probabilità, la divergenza KL valuta quanto una distribuzione di probabilità <span class="math notranslate nohighlight">\( Q \)</span> differisca da un’altra distribuzione di riferimento <span class="math notranslate nohighlight">\( P \)</span>. Entrambe le distribuzioni devono descrivere la stessa variabile aleatoria <span class="math notranslate nohighlight">\( X \)</span>.</p>
<section id="calcolo-della-divergenza-kl">
<h4>Calcolo della Divergenza KL<a class="headerlink" href="#calcolo-della-divergenza-kl" title="Link to this heading">#</a></h4>
<p>Supponiamo che la variabile casuale <span class="math notranslate nohighlight">\( X \)</span> segua la distribuzione <span class="math notranslate nohighlight">\( P \)</span>. L’entropia di Shannon, che quantifica la sorpresa media risultante dall’osservazione di esiti distribuiti secondo <span class="math notranslate nohighlight">\( P \)</span>, si calcola come:</p>
<div class="math notranslate nohighlight">
\[
H(P) = -\sum_x p(x) \log(p(x)).
\]</div>
<p>Per valutare quanto sarebbe sorprendente osservare <span class="math notranslate nohighlight">\( P \)</span> attraverso la lente di una distribuzione diversa <span class="math notranslate nohighlight">\( Q \)</span>, calcoliamo l’entropia incrociata, definita come:</p>
<div class="math notranslate nohighlight">
\[
H(P, Q) = -\sum_x p(x) \log(q(x)).
\]</div>
<p>Questa misura rappresenta la sorpresa attesa se utilizzassimo <span class="math notranslate nohighlight">\( Q \)</span> anziché <span class="math notranslate nohighlight">\( P \)</span> per descrivere la variabile aleatoria <span class="math notranslate nohighlight">\( X \)</span>.</p>
<p>La divergenza KL, che è la differenza tra l’entropia di <span class="math notranslate nohighlight">\( P \)</span> e l’entropia incrociata tra <span class="math notranslate nohighlight">\( P \)</span> e <span class="math notranslate nohighlight">\( Q \)</span>, si esprime come:</p>
<div class="math notranslate nohighlight">
\[
D_{KL}(P \parallel Q) = \sum_x p(x) \big(\log(p(x)) - \log(q(x))\big).
\]</div>
<p>Alternativamente, la formula precedente può essere riscritta utilizzando il rapporto tra i logaritmi:</p>
<div class="math notranslate nohighlight">
\[
D_{KL}(P \parallel Q) = \sum_x p(x) \log \left(\frac{p(x)}{q(x)}\right).
\]</div>
<p>In queste formule, <span class="math notranslate nohighlight">\( \left(\log(p(x)) - \log(q(x))\right) \)</span> rappresenta il «costo» di sorpresa per ciascun esito <span class="math notranslate nohighlight">\( x \)</span>, ponderato dalla probabilità <span class="math notranslate nohighlight">\( p(x) \)</span> di tale esito secondo la distribuzione originale <span class="math notranslate nohighlight">\( P \)</span>. Questo costo quantifica quanto <span class="math notranslate nohighlight">\( Q \)</span> sia inadeguata a modellare o descrivere <span class="math notranslate nohighlight">\( P \)</span>.</p>
<p>La divergenza KL quantifica «quanto siamo sorpresi» nell’utilizzare <span class="math notranslate nohighlight">\( Q \)</span> per prevedere eventi distribuiti secondo <span class="math notranslate nohighlight">\( P \)</span> e riflette l’informazione che viene «persa» quando <span class="math notranslate nohighlight">\( Q \)</span> è usata al posto di <span class="math notranslate nohighlight">\( P \)</span>.</p>
<p>In conclusione, la divergenza KL si basa su due misure fondamentali:</p>
<ul class="simple">
<li><p><strong>Entropia di <span class="math notranslate nohighlight">\( P \)</span></strong>: Misura l’incertezza interna di <span class="math notranslate nohighlight">\( P \)</span>.</p></li>
<li><p><strong>Entropia incrociata tra <span class="math notranslate nohighlight">\( P \)</span> e <span class="math notranslate nohighlight">\( Q \)</span></strong>: Quantifica l’incertezza quando <span class="math notranslate nohighlight">\( Q \)</span> è utilizzata per stimare <span class="math notranslate nohighlight">\( P \)</span>.</p></li>
</ul>
<p>Così, la divergenza KL rappresenta la differenza tra l’entropia di <span class="math notranslate nohighlight">\( P \)</span> e l’entropia incrociata tra <span class="math notranslate nohighlight">\( P \)</span> e <span class="math notranslate nohighlight">\( Q \)</span>, e mette in evidenza quanto l’uso di <span class="math notranslate nohighlight">\( Q \)</span> al posto di <span class="math notranslate nohighlight">\( P \)</span> incrementi l’incertezza o la sorpresa.</p>
<p>Per fare un esempio, supponiamo che <span class="math notranslate nohighlight">\( P \)</span> e <span class="math notranslate nohighlight">\( Q \)</span> siano due distribuzioni di probabilità su un insieme finito di possibili esiti, ad esempio {0, 1, 2}. Per semplicità, consideriamo che <span class="math notranslate nohighlight">\( P \)</span> e <span class="math notranslate nohighlight">\( Q \)</span> siano definite come segue:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( P \)</span> è la distribuzione «vera»: <span class="math notranslate nohighlight">\( P = [0.1, 0.6, 0.3] \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( Q \)</span> è una distribuzione alternativa che usiamo per la stima: <span class="math notranslate nohighlight">\( Q = [0.2, 0.5, 0.3] \)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Definizione delle distribuzioni</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>

<span class="c1"># Calcolo della divergenza KL da P a Q</span>
<span class="n">KL_divergence</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">kl_div</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">Q</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Divergenza KL da P a Q: </span><span class="si">{</span><span class="n">KL_divergence</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Divergenza KL da P a Q: 0.0401
</pre></div>
</div>
</div>
</div>
<p>Nel codice precedente, <code class="docutils literal notranslate"><span class="pre">kl_div(P,</span> <span class="pre">Q)</span></code> calcola la divergenza KL elemento per elemento dell’array. Essa calcola <span class="math notranslate nohighlight">\(\sum_x p(x) \log \left(\frac{p(x)}{q(x)}\right)\)</span> per ogni esito <span class="math notranslate nohighlight">\( x \)</span>, che è esattamente il termine <span class="math notranslate nohighlight">\( p(x) \log \left(\frac{p(x)}{q(x)}\right) \)</span> descritto nella formula della divergenza KL. Utilizziamo poi <code class="docutils literal notranslate"><span class="pre">np.sum</span></code> per sommare tutti i contributi individuali e ottenere il valore totale della divergenza KL.</p>
<p>Questo esempio fornisce un calcolo diretto della divergenza KL tra due distribuzioni, mostrando come una distribuzione <span class="math notranslate nohighlight">\( Q \)</span> possa essere inadeguata nel modellare una distribuzione <span class="math notranslate nohighlight">\( P \)</span>, con un focus sul «costo» di sorpresa per ogni esito.</p>
<p>In un due altri esempi, rendiamo via via <span class="math notranslate nohighlight">\(Q\)</span> più diverso da <span class="math notranslate nohighlight">\(P\)</span>. Notiamo come la divergenza KL aumenta.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">])</span>
<span class="n">KL_divergence</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">kl_div</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">Q</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Divergenza KL da P a Q: </span><span class="si">{</span><span class="n">KL_divergence</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Divergenza KL da P a Q: 0.2444
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
<span class="n">KL_divergence</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">kl_div</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">Q</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Divergenza KL da P a Q: </span><span class="si">{</span><span class="n">KL_divergence</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Divergenza KL da P a Q: 0.5663
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="applicazione-della-divergenza-kl-nella-selezione-di-modelli">
<h3>Applicazione della Divergenza KL nella Selezione di Modelli<a class="headerlink" href="#applicazione-della-divergenza-kl-nella-selezione-di-modelli" title="Link to this heading">#</a></h3>
<p>La divergenza di Kullback-Leibler (KL) è un indice fondamentale nella selezione di modelli statistici. L’obiettivo è identificare il modello <span class="math notranslate nohighlight">\( Q \)</span> che minimizza <span class="math notranslate nohighlight">\( D_{KL}(P \parallel Q) \)</span>, ovvero ridurre al minimo la differenza <span class="math notranslate nohighlight">\( H(P) - H(P, Q) \)</span>. Questo significa minimizzare l’errore introdotto nell’approssimare la distribuzione vera <span class="math notranslate nohighlight">\( P \)</span> con il modello <span class="math notranslate nohighlight">\( Q \)</span>.</p>
<section id="proprieta-importanti">
<h4>Proprietà Importanti<a class="headerlink" href="#proprieta-importanti" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Non-negatività:</strong> <span class="math notranslate nohighlight">\( D_{KL}(P \parallel Q) \geq 0 \)</span>. Il valore è zero solamente quando <span class="math notranslate nohighlight">\( P \)</span> e <span class="math notranslate nohighlight">\( Q \)</span> sono identiche, indicando una perfetta corrispondenza.</p></li>
<li><p><strong>Asimmetria:</strong> <span class="math notranslate nohighlight">\( D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P) \)</span>. Questa proprietà evidenzia che la «distanza» percepita dal modello <span class="math notranslate nohighlight">\( Q \)</span> verso <span class="math notranslate nohighlight">\( P \)</span> non è equivalente se misurata nella direzione inversa.</p></li>
</ul>
</section>
<section id="selezione-dei-modelli-statistici">
<h4>Selezione dei Modelli Statistici<a class="headerlink" href="#selezione-dei-modelli-statistici" title="Link to this heading">#</a></h4>
<p>Nel contesto della selezione di modelli statistici, l’intento principale è quello di scegliere il modello <span class="math notranslate nohighlight">\( Q \)</span> che riduca al minimo la divergenza KL rispetto alla distribuzione «vera» <span class="math notranslate nohighlight">\( P \)</span> dei dati. Tuttavia, la distribuzione <span class="math notranslate nohighlight">\( P \)</span> è spesso sconosciuta o non direttamente osservabile.</p>
<p>Di fronte a tale incertezza, i ricercatori e gli statistici utilizzano criteri approssimativi per stimare indirettamente la divergenza KL. Tra questi, il Criterio di Informazione di Akaike (AIC) e il Criterio di Informazione Bayesiano (BIC) sono particolarmente utili, in quanto valutano la bontà di adattamento del modello insieme alla sua complessità:</p>
<ul>
<li><p><strong>Criterio di Informazione di Akaike (AIC):</strong></p>
<div class="math notranslate nohighlight">
\[
  AIC = 2k - 2\ln(L),
  \]</div>
<p>dove <span class="math notranslate nohighlight">\( k \)</span> rappresenta il numero di parametri nel modello e <span class="math notranslate nohighlight">\( L \)</span> è la log-verosimiglianza massima ottenuta dal modello. Un valore più basso di AIC indica una minor perdita di informazione, suggerendo un modello preferibile.</p>
</li>
</ul>
<p>Consideriamo un esempio numerico per confrontare AIC con la divergenza KL. Supponiamo di avere un set di dati e due modelli statistici: il primo modello si adatta bene ai dati (modello vero), mentre il secondo è un po” più distante dalla realtà (modello alternativo, ne considereremo 5). Calcoleremo la divergenza KL tra le distribuzioni previste da questi modelli e il Criterio di Informazione di Akaike per valutare la qualità di adattamento dei modelli.</p>
<p>Per questo esempio, supponiamo di avere un set di dati e due modelli statistici: il primo modello si adatta bene ai dati (modello vero), mentre il secondo è un po” più distante dalla realtà (modello alternativo). Calcoleremo la divergenza KL tra le distribuzioni previste da questi modelli e il Criterio di Informazione di Akaike per valutare la qualità di adattamento dei modelli.</p>
<ul class="simple">
<li><p><strong>Generazione dei dati</strong>: Supponiamo che i dati siano generati da una distribuzione normale con media vera <span class="math notranslate nohighlight">\( \mu = 0 \)</span> e deviazione standard <span class="math notranslate nohighlight">\( \sigma = 1 \)</span>.</p></li>
<li><p><strong>Modello Vero</strong>: Assumiamo che il modello vero conosca i parametri della distribuzione.</p></li>
<li><p><strong>Modello Alternativo</strong>: Assumiamo che questo modello abbia una deviazione standard leggermente diversa (considereremo 5 modelli diversi<span class="math notranslate nohighlight">\(: \sigma = 1.5 fino a 5.0 \)</span>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generazione dei dati simulati</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Parametri del modello vero</span>
<span class="n">mu_true</span><span class="p">,</span> <span class="n">sigma_true</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>

<span class="c1"># Variazione di sigma_alt</span>
<span class="n">sigma_alts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">KL_divergences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">AIC_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Calcolo della divergenza KL e AIC per ogni sigma_alt</span>
<span class="k">for</span> <span class="n">sigma_alt</span> <span class="ow">in</span> <span class="n">sigma_alts</span><span class="p">:</span>
    <span class="n">p_true</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mu_true</span><span class="p">,</span> <span class="n">sigma_true</span><span class="p">)</span>
    <span class="n">p_alt</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mu_true</span><span class="p">,</span> <span class="n">sigma_alt</span><span class="p">)</span>
    <span class="n">KL_divergence</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p_true</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_true</span> <span class="o">/</span> <span class="n">p_alt</span><span class="p">))</span>
    <span class="n">KL_divergences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">KL_divergence</span><span class="p">)</span>

    <span class="n">log_likelihood_alt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mu_true</span><span class="p">,</span> <span class="n">sigma_alt</span><span class="p">)))</span>
    <span class="n">AIC_alt</span> <span class="o">=</span> <span class="p">(</span>
        <span class="mi">2</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">log_likelihood_alt</span>
    <span class="p">)</span>  <span class="c1"># 2 parametri (mu e sigma), nessuna esponenziale</span>
    <span class="n">AIC_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIC_alt</span><span class="p">)</span>

<span class="c1"># Creazione del grafico</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigma_alts</span><span class="p">,</span> <span class="n">KL_divergences</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Divergenza KL&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigma_alts</span><span class="p">,</span> <span class="n">AIC_values</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;AIC&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sigma Alternativo&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Valore&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Divergenza KL e AIC al variare di Sigma Alternativo&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../_images/d6b13d0ef8f6bbb86852a2999629754928b757badb4b66fadcf0a39b78f2e017.png"><img alt="../_images/d6b13d0ef8f6bbb86852a2999629754928b757badb4b66fadcf0a39b78f2e017.png" src="../_images/d6b13d0ef8f6bbb86852a2999629754928b757badb4b66fadcf0a39b78f2e017.png" style="width: 1011px; height: 511px;" /></a>
</div>
</div>
<p>Si vede che, anche se la scala di misura è diversa tra la divergenza KL e il criterio AIC, all’aumentare della differenza tra la distribuzione vera <span class="math notranslate nohighlight">\(P\)</span> e la distribuzione alternativa <span class="math notranslate nohighlight">\(Q\)</span>, entrambi aumentano.</p>
<ul>
<li><p><strong>Criterio di Informazione Bayesiano (BIC):</strong></p>
<div class="math notranslate nohighlight">
\[
  BIC = \ln(n)k - 2\ln(L),
  \]</div>
<p>dove <span class="math notranslate nohighlight">\( n \)</span> è il numero di osservazioni. Il BIC impone una penalità maggiore per l’incremento dei parametri, rendendolo particolarmente adeguato per dataset di grandi dimensioni.</p>
</li>
</ul>
<p>Entrambi i criteri, AIC e BIC, mirano a bilanciare la complessità del modello con la sua capacità di spiegare i dati, cercando di prevenire il sovra-adattamento. Utilizzando queste metriche, gli analisti possono effettuare scelte più informate nella selezione dei modelli, avvicinandosi all’obiettivo di minimizzare la divergenza KL tra il modello prescelto e la vera distribuzione sottostante dei dati.</p>
</section>
</section>
</section>
<section id="riflessioni-conclusive">
<h2>Riflessioni Conclusive<a class="headerlink" href="#riflessioni-conclusive" title="Link to this heading">#</a></h2>
<p>In questo capitolo, abbiamo esaminato il concetto di entropia, evidenziando il suo ruolo fondamentale nel quantificare l’incertezza all’interno delle distribuzioni di probabilità. Abbiamo anche affrontato la questione di come l’entropia possa essere impiegata per valutare la «distanza» tra un modello teorico e i dati reali. A tale scopo, abbiamo introdotto la divergenza di Kullback-Leibler (KL), una misura che quantifica le discrepanze tra due distribuzioni di probabilità.</p>
<p>Nel capitolo successivo, approfondiremo ulteriormente il tema della divergenza KL. Esploreremo come questo strumento possa essere utilizzato per confrontare modelli teorici con dati empirici e ci concentreremo su come possa fornirci una comprensione più dettagliata dell’adattamento di un modello alla realtà che intende rappresentare. Questa esplorazione ci permetterà di valutare più accuratamente la validità e la generalizzabilità dei modelli scientifici nel loro tentativo di catturare e interpretare la complessità dei fenomeni oggetto di studio.</p>
</section>
<section id="informazioni-sull-ambiente-di-sviluppo">
<h2>Informazioni sull’Ambiente di Sviluppo<a class="headerlink" href="#informazioni-sull-ambiente-di-sviluppo" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> -n -u -v -iv -w -m
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Last updated: Sun Jun 16 2024

Python implementation: CPython
Python version       : 3.12.3
IPython version      : 8.25.0

Compiler    : Clang 16.0.6 
OS          : Darwin
Release     : 23.4.0
Machine     : arm64
Processor   : arm
CPU cores   : 8
Architecture: 64bit

arviz     : 0.18.0
numpy     : 1.26.4
pandas    : 2.2.2
matplotlib: 3.8.4
scipy     : 1.13.1

Watermark: 2.4.3
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_5"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="05_26_stan_mediation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">precedente</p>
        <p class="prev-next-title">Modello di mediazione con Stan</p>
      </div>
    </a>
    <a class="right-next"
       href="05_31_kl.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">successivo</p>
        <p class="prev-next-title">La divergenza di Kullback-Leibler</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contenuti
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparazione-del-notebook">Preparazione del Notebook</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-generalizzabilita-dei-modelli-e-il-metodo-scientifico">La Generalizzabilità dei Modelli e il Metodo Scientifico</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cos-e-l-entropia-dell-informazione">Cos’è l’Entropia dell’Informazione?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additivita-dell-entropia-per-eventi-indipendenti">Additività dell’Entropia per Eventi Indipendenti</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropia-di-variabili-casuali">Entropia di Variabili Casuali</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#entropia-di-una-variabile-casuale-discreta">Entropia di una Variabile Casuale Discreta</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#entropia-di-una-variabile-casuale-continua">Entropia di una Variabile Casuale Continua</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applicazioni-psicologiche">Applicazioni Psicologiche</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#divergenza-di-kullback-leibler-uno-strumento-per-confrontare-distribuzioni-probabilistiche">Divergenza di Kullback-Leibler: Uno Strumento per Confrontare Distribuzioni Probabilistiche</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calcolo-della-divergenza-kl">Calcolo della Divergenza KL</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applicazione-della-divergenza-kl-nella-selezione-di-modelli">Applicazione della Divergenza KL nella Selezione di Modelli</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#proprieta-importanti">Proprietà Importanti</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#selezione-dei-modelli-statistici">Selezione dei Modelli Statistici</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#riflessioni-conclusive">Riflessioni Conclusive</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#informazioni-sull-ambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Di Corrado Caudek
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>